{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb06a97c",
   "metadata": {},
   "source": [
    "## MISA (2024-2025)\n",
    "- Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia redémarrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
    "\n",
    "- Izay misy hoe `YOUR CODE HERE` na `YOUR ANSWER HERE` ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b9364",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Segmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "* [regression logistique](https://fr.wikipedia.org/wiki/R%C3%A9gression_logistique)\n",
    "* [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "* [Log Loss / Cross-Entropy - Wikipedia](https://en.wikipedia.org/wiki/Cross-entropy)\n",
    "* [Mean squred error](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "* [regression de ridge](https://en.wikipedia.org/wiki/Ridge_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36596133",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "ab42c33c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfef89ef4ca46a14c477cc32b17222e5",
     "grade": false,
     "grade_id": "cell-f49a65d13436731a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec4298",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "731a18bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be473c46fc6258ee11c20953d99e53e3",
     "grade": false,
     "grade_id": "cell-a28fb0bb09ca56b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X_train1, y_train1 = data.data, data.target\n",
    "w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n",
    "b1 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "f07dea71",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf223ff00ec5a3418053e6125b7f9988",
     "grade": false,
     "grade_id": "cell-f798c8cb8fdf24eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_loss_naive(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    MSE loss function WITH FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)  # Initialiser dw avec la même forme que w\n",
    "    db = 0.0\n",
    "    \n",
    "    n = len(X)  # Nombre d'exemples\n",
    "    \n",
    "    # Calcul de la perte MSE et des gradients\n",
    "    for i in range(n):\n",
    "        y_pred = 0.0\n",
    "        # Calcul de la prédiction pour l'exemple i\n",
    "        for j in range(len(w)):  # Calcul de X[i] . w\n",
    "            y_pred += X[i][j] * w[j]\n",
    "        y_pred += b\n",
    "        \n",
    "        # Calcul de la perte pour cet exemple\n",
    "        loss += (y[i] - y_pred) ** 2\n",
    "        \n",
    "        # Calcul des gradients pour cet exemple\n",
    "        for j in range(len(w)):  # Gradient par rapport aux poids\n",
    "            dw[j] += -2 * X[i][j] * (y[i] - y_pred)\n",
    "        db += -2 * (y[i] - y_pred)  # Gradient par rapport au biais\n",
    "\n",
    "    # Moyenne sur tous les exemples\n",
    "    loss /= n\n",
    "    dw /= n\n",
    "    db /= n\n",
    "    \n",
    "    # Ajouter la régularisation L2 si alpha > 0\n",
    "    if alpha > 0:\n",
    "        for j in range(len(w)):\n",
    "            loss += alpha * w[j] ** 2  # Ajouter la régularisation\n",
    "            dw[j] += 2 * alpha * w[j]  # Gradient de la régularisation\n",
    "\n",
    "    return loss, dw, np.array(db).reshape(1,)  # Retour avec le format correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddacc5d",
   "metadata": {},
   "source": [
    "### Naive Linear regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "86cdcedc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5dde00a4ca07a123f6a98618ae81a98",
     "grade": true,
     "grade_id": "cell-84268496cd13d9b2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13253/3270852306.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dw[j] += -2 * X[i][j] * (y[i] - y_pred)\n",
      "/tmp/ipykernel_13253/3270852306.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dw[j] += -2 * X[i][j] * (y[i] - y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  3.12815550455564e-16\n",
      "Gradient check w\n",
      "numerical: -4.296088 analytic: -4.296087, relative error: 4.037504e-08\n",
      "numerical: -0.315453 analytic: -0.315453, relative error: 7.845173e-08\n",
      "numerical: -1.275042 analytic: -1.275043, relative error: 5.000248e-07\n",
      "numerical: -1.275042 analytic: -1.275043, relative error: 5.000248e-07\n",
      "numerical: -1.275042 analytic: -1.275043, relative error: 5.000248e-07\n",
      "numerical: -1.553186 analytic: -1.553188, relative error: 3.444153e-07\n",
      "numerical: -4.145419 analytic: -4.145418, relative error: 1.134015e-07\n",
      "numerical: -2.801912 analytic: -2.801913, relative error: 1.067256e-07\n",
      "numerical: -1.275042 analytic: -1.275043, relative error: 5.000248e-07\n",
      "numerical: -3.234111 analytic: -3.234109, relative error: 2.558277e-07\n",
      "numerical: -3.234111 analytic: -3.234109, relative error: 2.558277e-07\n",
      "numerical: -3.153315 analytic: -3.153316, relative error: 1.573529e-07\n",
      "numerical: -3.234111 analytic: -3.234109, relative error: 2.558277e-07\n",
      "numerical: -4.296088 analytic: -4.296087, relative error: 4.037504e-08\n",
      "numerical: -0.315453 analytic: -0.315453, relative error: 7.845173e-08\n",
      "Gradient check bias\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db438d3f",
   "metadata": {},
   "source": [
    "### Naive Ridge regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "36465817",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9354044aabf6361d6faeec37891a6342",
     "grade": true,
     "grade_id": "cell-a8e942346b61f083",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13253/3270852306.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dw[j] += -2 * X[i][j] * (y[i] - y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -3.234130 analytic: -3.234128, relative error: 2.714084e-07\n",
      "numerical: -4.145478 analytic: -4.145478, relative error: 1.126765e-07\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: -1.274901 analytic: -1.274902, relative error: 4.730617e-07\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: -1.553317 analytic: -1.553318, relative error: 3.389340e-07\n",
      "numerical: -1.376010 analytic: -1.376010, relative error: 3.838528e-08\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: 2.891891 analytic: 2.891891, relative error: 5.103281e-08\n",
      "numerical: -4.296216 analytic: -4.296216, relative error: 4.085456e-08\n",
      "numerical: 2.891891 analytic: 2.891891, relative error: 5.103281e-08\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: -1.553317 analytic: -1.553318, relative error: 3.389340e-07\n",
      "numerical: -1.376010 analytic: -1.376010, relative error: 3.838528e-08\n",
      "numerical: -1.553317 analytic: -1.553318, relative error: 3.389340e-07\n",
      "Gradient check bias\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n",
      "numerical: -304.266903 analytic: -304.266905, relative error: 2.722017e-09\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "5c6b011f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d706dfd270eed5599cf05586623981f",
     "grade": false,
     "grade_id": "cell-5470d6ef89b09593",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_loss_vectorized(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    MSE loss function WITHOUT FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    # Nombre d'exemples\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calcul des prédictions\n",
    "    y_pred = X @ w + b\n",
    "    \n",
    "    # Calcul de la perte MSE\n",
    "    loss = np.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    # Ajouter la régularisation L2 si alpha > 0\n",
    "    if alpha > 0:\n",
    "        loss += alpha * np.sum(w ** 2)  # Ajout de la régularisation\n",
    "\n",
    "    # Calcul des gradients\n",
    "    dw = 2 * X.T @ (y_pred - y) / n + 2 * alpha * w # Gradient par rapport aux poids\n",
    "    db = 2 * np.sum(y_pred - y) / n  # Gradient par rapport au biais\n",
    "    \n",
    "    return loss, dw, np.array(db).reshape(1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bff00",
   "metadata": {},
   "source": [
    "### Vectorised Linear regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "05dd3b4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34f449112eb0d9f69b052a5c2cdc3379",
     "grade": true,
     "grade_id": "cell-6b4e54d894e2ebee",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  0.0\n",
      "Gradient check w\n",
      "numerical: -1.376393 analytic: -1.376393, relative error: 5.684287e-08\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 6.548137e-08\n",
      "numerical: -2.801913 analytic: -2.801913, relative error: 5.557325e-08\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 6.548137e-08\n",
      "numerical: -3.234109 analytic: -3.234109, relative error: 2.730190e-09\n",
      "numerical: -0.315453 analytic: -0.315453, relative error: 7.845173e-08\n",
      "numerical: -0.315453 analytic: -0.315453, relative error: 7.845173e-08\n",
      "numerical: 2.892060 analytic: 2.892060, relative error: 2.015473e-08\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 6.548137e-08\n",
      "numerical: 2.892060 analytic: 2.892060, relative error: 2.015473e-08\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 6.548137e-08\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 6.548137e-08\n",
      "numerical: -3.234109 analytic: -3.234109, relative error: 2.730190e-09\n",
      "numerical: -3.153317 analytic: -3.153316, relative error: 1.570198e-08\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 3.702697e-09\n",
      "Gradient check bias\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  0.0\n",
      "Gradient check w\n",
      "numerical: 2.892060 analytic: 2.892060, relative error: 2.015473e-08\n",
      "numerical: -4.296087 analytic: -4.296087, relative error: 2.313587e-08\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 3.702697e-09\n",
      "numerical: -4.296087 analytic: -4.296087, relative error: 2.313587e-08\n",
      "numerical: -3.153317 analytic: -3.153316, relative error: 1.570198e-08\n",
      "numerical: -1.275044 analytic: -1.275043, relative error: 1.419500e-07\n",
      "numerical: -4.296087 analytic: -4.296087, relative error: 2.313587e-08\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 3.702697e-09\n",
      "numerical: -2.801913 analytic: -2.801913, relative error: 5.557325e-08\n",
      "numerical: -4.296087 analytic: -4.296087, relative error: 2.313587e-08\n",
      "numerical: -4.145418 analytic: -4.145418, relative error: 3.702697e-09\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 6.548137e-08\n",
      "numerical: -1.376393 analytic: -1.376393, relative error: 5.684287e-08\n",
      "numerical: 2.892060 analytic: 2.892060, relative error: 2.015473e-08\n",
      "numerical: -0.315453 analytic: -0.315453, relative error: 7.845173e-08\n",
      "Gradient check bias\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8575f",
   "metadata": {},
   "source": [
    "### Vectorized ridge regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "6aa107f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62b9bee0f7d16658d0e444788e4b23f3",
     "grade": true,
     "grade_id": "cell-76aafc86c4b9d590",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -4.296216 analytic: -4.296216, relative error: 2.265445e-08\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: -1.274903 analytic: -1.274902, relative error: 1.689841e-07\n",
      "numerical: -1.553319 analytic: -1.553318, relative error: 7.092821e-08\n",
      "numerical: -4.145478 analytic: -4.145478, relative error: 2.979284e-09\n",
      "numerical: -1.553319 analytic: -1.553318, relative error: 7.092821e-08\n",
      "numerical: -3.153363 analytic: -3.153363, relative error: 1.894784e-08\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "numerical: -4.296216 analytic: -4.296216, relative error: 2.265445e-08\n",
      "numerical: -4.145478 analytic: -4.145478, relative error: 2.979284e-09\n",
      "numerical: -4.145478 analytic: -4.145478, relative error: 2.979284e-09\n",
      "numerical: -1.274903 analytic: -1.274902, relative error: 1.689841e-07\n",
      "numerical: -4.296216 analytic: -4.296216, relative error: 2.265445e-08\n",
      "numerical: -0.315114 analytic: -0.315114, relative error: 4.515557e-08\n",
      "Gradient check bias\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n",
      "numerical: -304.266904 analytic: -304.266905, relative error: 6.296231e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7ae33",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "bb6d1eb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6517f859f0631f5bfc11b98329aeaec7",
     "grade": false,
     "grade_id": "cell-2a1d2d4fed4971bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train2, y_train2 = data.data, data.target\n",
    "w2 = np.random.randn(X_train2.shape[1]) * 0.0001\n",
    "b2 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e5d16",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "4690c331",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6f285b70459722184931efd2abd3bb6",
     "grade": false,
     "grade_id": "cell-879a7232cd873202",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_naive(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    Log loss function WITH FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    n = len(X)  # Nombre d'exemples\n",
    "    \n",
    "    # Calcul de la perte logistique et des gradients\n",
    "    for i in range(n):\n",
    "        # Calcul de la prédiction pour l'exemple i\n",
    "        y_pred = 0.0\n",
    "        for j in range(len(w)):  # Calcul de X[i] . w\n",
    "            y_pred += X[i][j] * w[j]\n",
    "        y_pred += b\n",
    "        \n",
    "        # Application de la fonction sigmoïde\n",
    "        sigmoid_pred = 1 / (1 + np.exp(-y_pred))\n",
    "        \n",
    "        # Calcul de la perte pour cet exemple\n",
    "        loss += -y[i] * np.log(sigmoid_pred) - (1 - y[i]) * np.log(1 - sigmoid_pred)\n",
    "        \n",
    "        # Calcul des gradients pour cet exemple\n",
    "        for j in range(len(w)):  # Gradient par rapport aux poids\n",
    "            dw[j] += (sigmoid_pred - y[i]) * X[i][j]\n",
    "        db += sigmoid_pred - y[i]  # Gradient par rapport au biais\n",
    "\n",
    "    # Moyenne sur tous les exemples\n",
    "    loss /= n\n",
    "    dw /= n\n",
    "    db /= n\n",
    "    \n",
    "    # Ajouter la régularisation L2 si alpha > 0\n",
    "    if alpha > 0:\n",
    "        for j in range(len(w)):\n",
    "            loss += alpha * w[j] ** 2  # Ajouter la régularisation à la perte\n",
    "            dw[j] += 2 * alpha * w[j]  # Ajouter la régularisation au gradient\n",
    "\n",
    "    return loss, dw, np.array(db).reshape(1,)  # Retour avec le format correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "f54b1bca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e1054080e6f097693d1b07c22c80e10",
     "grade": true,
     "grade_id": "cell-edbf23327a9bc955",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13253/4212622304.py:32: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dw[j] += (sigmoid_pred - y[i]) * X[i][j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  1.561596465087343e-16\n",
      "Gradient check w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13253/1836945256.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"numerical: %f analytic: %f, relative error: %e\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.000697 analytic: -0.000697, relative error: 5.127431e-10\n",
      "numerical: -0.104564 analytic: -0.104564, relative error: 1.760468e-10\n",
      "numerical: 0.574468 analytic: 0.574468, relative error: 2.666072e-09\n",
      "numerical: 0.574468 analytic: 0.574468, relative error: 2.666072e-09\n",
      "numerical: 0.025315 analytic: 0.025315, relative error: 3.793623e-10\n",
      "numerical: 142.247903 analytic: 142.249327, relative error: 5.006893e-06\n",
      "numerical: 0.007182 analytic: 0.007182, relative error: 1.661350e-09\n",
      "numerical: -0.010610 analytic: -0.010610, relative error: 8.995161e-10\n",
      "numerical: 9.587780 analytic: 9.587781, relative error: 4.512927e-08\n",
      "numerical: -0.708580 analytic: -0.708580, relative error: 2.396063e-09\n",
      "numerical: -0.010610 analytic: -0.010610, relative error: 8.995161e-10\n",
      "numerical: -0.005324 analytic: -0.005324, relative error: 1.809466e-09\n",
      "numerical: -0.708580 analytic: -0.708580, relative error: 2.396063e-09\n",
      "numerical: 0.046509 analytic: 0.046509, relative error: 1.685480e-10\n",
      "numerical: -0.001756 analytic: -0.001756, relative error: 1.314860e-08\n",
      "Gradient check bias\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n"
     ]
    }
   ],
   "source": [
    "y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n",
    "y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n",
    "sk_loss = log_loss(y_train2, y_pred)\n",
    "\n",
    "loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e8516",
   "metadata": {},
   "source": [
    "### Naive with regulariztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "79f6f9a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4df065b17ae3d8eb650bfc356476512f",
     "grade": true,
     "grade_id": "cell-2482505d2f813068",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13253/4212622304.py:32: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dw[j] += (sigmoid_pred - y[i]) * X[i][j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13253/1836945256.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"numerical: %f analytic: %f, relative error: %e\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000999 analytic: 0.000999, relative error: 2.094554e-08\n",
      "numerical: -0.734764 analytic: -0.734764, relative error: 5.414611e-09\n",
      "numerical: -0.001033 analytic: -0.001033, relative error: 1.134618e-09\n",
      "numerical: 0.000999 analytic: 0.000999, relative error: 2.094554e-08\n",
      "numerical: -0.012104 analytic: -0.012104, relative error: 3.639682e-10\n",
      "numerical: 0.025465 analytic: 0.025465, relative error: 3.681944e-10\n",
      "numerical: 0.000999 analytic: 0.000999, relative error: 2.094554e-08\n",
      "numerical: 0.007072 analytic: 0.007072, relative error: 1.988774e-09\n",
      "numerical: 0.000821 analytic: 0.000821, relative error: 4.288783e-09\n",
      "numerical: -0.001033 analytic: -0.001033, relative error: 1.134618e-09\n",
      "numerical: -0.001033 analytic: -0.001033, relative error: 1.134618e-09\n",
      "numerical: 0.011363 analytic: 0.011363, relative error: 1.160073e-10\n",
      "numerical: 0.574561 analytic: 0.574561, relative error: 2.666015e-09\n",
      "numerical: 0.151503 analytic: 0.151503, relative error: 5.871658e-09\n",
      "numerical: 0.016894 analytic: 0.016894, relative error: 4.634933e-11\n",
      "Gradient check bias\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 3.443341e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31564092",
   "metadata": {},
   "source": [
    "### Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "fc5084e9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "078679d141f4c139c6dab5ae6f45d8f9",
     "grade": false,
     "grade_id": "cell-e2043c379ba3cc3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_vectorized(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    Log loss function WITHOUT FOR LOOPS\n",
    "\n",
    "    Arguments:\n",
    "    w -- poids du modèle (vecteur de taille (d,))\n",
    "    b -- biais du modèle (scalaire)\n",
    "    X -- données d'entrée (matrice de taille (n, d))\n",
    "    y -- valeurs cibles réelles (vecteur de taille (n,))\n",
    "    alpha -- coefficient de régularisation L2 (scalaire, par défaut 0)\n",
    "    \n",
    "    Retourne:\n",
    "    loss -- perte logistique\n",
    "    dw -- gradient de la perte par rapport aux poids (vecteur de taille (d,))\n",
    "    db -- gradient de la perte par rapport au biais (scalaire)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[0]  # Nombre d'exemples\n",
    "    \n",
    "    # Calcul des prédictions (logits)\n",
    "    y_pred = X.dot(w) + b  # Produit matriciel X.w + b\n",
    "    \n",
    "    # Application de la fonction sigmoïde\n",
    "    sigmoid_pred = 1 / (1 + np.exp(-y_pred))\n",
    "    \n",
    "    # Calcul de la perte logistique (log loss)\n",
    "    loss = -np.mean(y * np.log(sigmoid_pred) + (1 - y) * np.log(1 - sigmoid_pred))\n",
    "    \n",
    "    # Ajouter la régularisation L2 si alpha > 0\n",
    "    if alpha > 0:\n",
    "        loss += alpha * np.sum(w ** 2)  # Régularisation sur les poids\n",
    "    \n",
    "    # Calcul des gradients\n",
    "    dw = X.T.dot(sigmoid_pred - y) / n  # Gradient par rapport aux poids w\n",
    "    \n",
    "    # Ajouter la régularisation L2 au gradient des poids\n",
    "    if alpha > 0:\n",
    "        dw += 2 * alpha * w  # Régularisation sur les gradients des poids\n",
    "    \n",
    "    db = np.sum(sigmoid_pred - y) / n   # Gradient par rapport au biais b\n",
    "    \n",
    "    # Retourner la perte et les gradients\n",
    "    return loss, dw, np.array(db).reshape(1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "c425e768",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71298dbbf1c5013b322fc1d32d99c506",
     "grade": true,
     "grade_id": "cell-19b05ba596d391cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  0.0\n",
      "Gradient check w\n",
      "numerical: 0.151743 analytic: 0.151743, relative error: 5.829276e-09\n",
      "numerical: -0.000218 analytic: -0.000218, relative error: 4.353361e-09\n",
      "numerical: -0.708580 analytic: -0.708580, relative error: 2.356892e-09\n",
      "numerical: 0.016846 analytic: 0.016846, relative error: 2.900927e-11\n",
      "numerical: 0.001193 analytic: 0.001193, relative error: 9.547380e-10\n",
      "numerical: -0.734952 analytic: -0.734952, relative error: 5.468686e-09\n",
      "numerical: -0.734952 analytic: -0.734952, relative error: 5.468686e-09\n",
      "numerical: -0.004158 analytic: -0.004158, relative error: 4.316839e-10\n",
      "numerical: -0.000218 analytic: -0.000218, relative error: 4.353361e-09\n",
      "numerical: -0.104564 analytic: -0.104564, relative error: 9.762140e-12\n",
      "numerical: 0.340628 analytic: 0.340628, relative error: 1.038116e-10\n",
      "numerical: 1.646976 analytic: 1.646977, relative error: 1.547210e-07\n",
      "numerical: 0.011274 analytic: 0.011274, relative error: 1.246484e-10\n",
      "numerical: 0.046509 analytic: 0.046509, relative error: 4.919240e-11\n",
      "numerical: 0.016846 analytic: 0.016846, relative error: 2.900927e-11\n",
      "Gradient check bias\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n"
     ]
    }
   ],
   "source": [
    "y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n",
    "y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n",
    "sk_loss = log_loss(y_train2, y_pred)\n",
    "\n",
    "loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7be5da",
   "metadata": {},
   "source": [
    "### Vectorized with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "a4e09127",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "487fb88e0a11371617655bfecc01e9e1",
     "grade": true,
     "grade_id": "cell-0c05b3e99335f4cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.012104 analytic: -0.012104, relative error: 3.239534e-10\n",
      "numerical: 74.653275 analytic: 74.653701, relative error: 2.858174e-06\n",
      "numerical: 0.016894 analytic: 0.016894, relative error: 1.179457e-10\n",
      "numerical: 0.000821 analytic: 0.000821, relative error: 2.468534e-09\n",
      "numerical: -0.005174 analytic: -0.005174, relative error: 6.118390e-10\n",
      "numerical: 0.046413 analytic: 0.046413, relative error: 2.175943e-12\n",
      "numerical: 0.000573 analytic: 0.000573, relative error: 8.446796e-09\n",
      "numerical: -0.104496 analytic: -0.104496, relative error: 6.201417e-13\n",
      "numerical: 142.248015 analytic: 142.249440, relative error: 5.006889e-06\n",
      "numerical: 0.021053 analytic: 0.021053, relative error: 1.738152e-10\n",
      "numerical: 0.025465 analytic: 0.025465, relative error: 6.778570e-11\n",
      "numerical: 0.016894 analytic: 0.016894, relative error: 1.179457e-10\n",
      "numerical: 0.000999 analytic: 0.000999, relative error: 1.282120e-09\n",
      "numerical: -0.005288 analytic: -0.005288, relative error: 1.629794e-10\n",
      "numerical: -0.010804 analytic: -0.010804, relative error: 7.978802e-11\n",
      "Gradient check bias\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n",
      "numerical: -0.083307 analytic: -0.083307, relative error: 1.116299e-11\n"
     ]
    }
   ],
   "source": [
    "loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403ea0",
   "metadata": {},
   "source": [
    "# Gradient descent for Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "5039af1c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb3642a12cc93b0863832e5acadc2ce8",
     "grade": false,
     "grade_id": "cell-076e6ebd0bdf09ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearModel():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None:  # Initialisation\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            # Tirage d'un mini-lot aléatoire\n",
    "            indices = np.random.choice(N, batch_size, replace=False)\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "                                                               \n",
    "            # Evaluer la perte et les gradients en utilisant la fonction de perte correspondante\n",
    "            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Mettre à jour les paramètres\n",
    "            self.w -= learning_rate * dw\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            # Afficher la perte toutes les 10000 itérations si verbose est True\n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(f\"iteration {it} / {num_iters}: loss {loss:.6f}\")\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Méthode générique pour la prédiction, utilisée dans les sous-classes \"\"\"\n",
    "        return X.dot(self.w) + self.b\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\" Méthode à implémenter dans les sous-classes \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class LinearRegressor(LinearModel):\n",
    "    \"\"\" Régression linéaire \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        \"\"\" Utilisation de la fonction mse_loss_vectorized pré-existante \"\"\"\n",
    "        return mse_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Prédiction pour la régression linéaire \"\"\"\n",
    "        return X.dot(self.w) + self.b\n",
    "\n",
    "\n",
    "class LogisticRegressor(LinearModel):\n",
    "    \"\"\" Régression logistique \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        \"\"\" Utilisation de la fonction log_loss_vectorized pré-existante \"\"\"\n",
    "        return log_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Retourner un vecteur de labels 0 ou 1 basé sur la régression logistique \"\"\"\n",
    "        y_pred = X.dot(self.w) + self.b\n",
    "        sigmoid_pred = 1 / (1 + np.exp(-y_pred))\n",
    "        return (sigmoid_pred >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0aa79",
   "metadata": {},
   "source": [
    "## Linear regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "86925e00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30f1c271b16efe670214663c10743937",
     "grade": true,
     "grade_id": "cell-92f36a3b387a4277",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 25639.985777\n",
      "iteration 10000 / 75000: loss 3589.225234\n",
      "iteration 20000 / 75000: loss 3085.272527\n",
      "iteration 30000 / 75000: loss 2515.969871\n",
      "iteration 40000 / 75000: loss 3384.985293\n",
      "iteration 50000 / 75000: loss 3280.957417\n",
      "iteration 60000 / 75000: loss 2418.706388\n",
      "iteration 70000 / 75000: loss 2859.704411\n",
      "MSE scikit-learn: 2859.6963475867506\n",
      "MSE gradient descent model : 2884.254180528585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sk_model = LinearRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train1, y_train1)\n",
    "sk_pred = sk_model.predict(X_train1)\n",
    "sk_mse = mean_squared_error(sk_pred, y_train1)\n",
    "\n",
    "model = LinearRegressor()\n",
    "model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n",
    "pred = model.predict(X_train1)\n",
    "mse = mean_squared_error(pred, y_train1)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE gradient descent model :\", mse)\n",
    "assert mse - sk_mse < 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c54d8",
   "metadata": {},
   "source": [
    "## Logistc regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "9219f422",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbd27c73b64e2b8344c8292a9b3f357e",
     "grade": true,
     "grade_id": "cell-20c0450a0bbb2b6c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 0.693999\n",
      "iteration 10000 / 75000: loss 0.103628\n",
      "iteration 20000 / 75000: loss 0.073980\n",
      "iteration 30000 / 75000: loss 0.071440\n",
      "iteration 40000 / 75000: loss 0.079441\n",
      "iteration 50000 / 75000: loss 0.033326\n",
      "iteration 60000 / 75000: loss 0.047670\n",
      "iteration 70000 / 75000: loss 0.034414\n",
      "Log-loss scikit-learn: 0.44341928598210933\n",
      "Log-loss gradiet descent model : 0.44341928598210933\n",
      "Error : 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "\n",
    "sk_model = LogisticRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train2, y_train2)\n",
    "sk_pred = sk_model.predict(X_train2)\n",
    "sk_log_loss = log_loss(sk_pred, y_train2)\n",
    "\n",
    "model = LogisticRegressor()\n",
    "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X_train2)\n",
    "model_log_loss = log_loss(pred, y_train2)\n",
    "\n",
    "print(\"Log-loss scikit-learn:\", sk_log_loss)\n",
    "print(\"Log-loss gradiet descent model :\", model_log_loss)\n",
    "print(\"Error :\", rel_error(sk_log_loss, model_log_loss))\n",
    "assert rel_error(sk_log_loss, model_log_loss) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f7431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
