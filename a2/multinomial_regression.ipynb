{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02b564ba",
      "metadata": {
        "id": "02b564ba"
      },
      "source": [
        "## MISA (2024-2025)\n",
        "- Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia red√©marrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
        "\n",
        "- Izay misy hoe `YOUR CODE HERE` na `YOUR ANSWER HERE` ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2e936e",
      "metadata": {
        "id": "de2e936e"
      },
      "source": [
        "## References\n",
        " * [Introduction to Logistic Regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\n",
        " * [Introduction to Logistic Regression](https://courses.lumenlearning.com/introstats1/chapter/introduction-to-logistic-regression/#:~:text=Logistic%20regression%20is%20a%20type,different%20from%20the%20normal%20distribution)\n",
        " * [Loss Functions in Machine Learning](https://www.geeksforgeeks.org/loss-functions-in-deep-learning)\n",
        " * [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
        " * [Softmax regression](https://en.wikipedia.org/wiki/Softmax_function)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7faa0c",
      "metadata": {
        "id": "1b7faa0c"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6676504c-57d7-441b-bdd2-70768689c593",
      "metadata": {
        "id": "6676504c-57d7-441b-bdd2-70768689c593"
      },
      "source": [
        "# Multinomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aebdce2a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "64aa12d94bb4e50e39182f704b3cfb32",
          "grade": false,
          "grade_id": "cell-333bdc4926f1cf56",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "aebdce2a"
      },
      "outputs": [],
      "source": [
        "from random import randrange\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, log_loss\n",
        "from sklearn.datasets import load_boston, load_diabetes, load_iris, load_digits\n",
        "from scipy.special import huber\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n",
        "    \"\"\"\n",
        "    sample a few random elements and only return numerical\n",
        "    in this dimensions\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape])\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)  # evaluate f(x + h)\n",
        "        x[ix] = oldval - h  # increment by h\n",
        "        fxmh = f(x)  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # reset\n",
        "\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
        "            abs(grad_numerical) + abs(grad_analytic)\n",
        "        )\n",
        "        print(\n",
        "            \"numerical: %f analytic: %f, relative error: %e\"\n",
        "            % (grad_numerical, grad_analytic, rel_error)\n",
        "        )\n",
        "        assert rel_error < error\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b194affb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fcf6c9bab4975732f03ce6c8215e72be",
          "grade": false,
          "grade_id": "cell-27c3352e3785ecdb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "b194affb"
      },
      "outputs": [],
      "source": [
        "data = load_iris()\n",
        "X_train2, y_train2 = data.data, data.target\n",
        "\n",
        "W = np.random.randn(X_train2.shape[1], 3) * 0.0001"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb164b2-6223-4d1f-8898-fdeb8d826b34",
      "metadata": {
        "id": "1eb164b2-6223-4d1f-8898-fdeb8d826b34"
      },
      "source": [
        "## Naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d8405c63",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "af6fecc0ab19bd2c392db4255a946d03",
          "grade": false,
          "grade_id": "cell-9049b2a8d3edaeaf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "d8405c63"
      },
      "outputs": [],
      "source": [
        "def softmax_loss_naive(W, X, y, alpha):\n",
        "    \"\"\"\n",
        "    Softmax loss function WITH FOR LOOPS\n",
        "\n",
        "    Inputs:\n",
        "    - W: array of shape (D, C) containing weights\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - alpha: (float) regularization\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; same shape as W\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    num_classes = W.shape[1]\n",
        "    num_train = X.shape[0]\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    for i in range(num_train):\n",
        "        # Compute the scores for each class\n",
        "        scores = X[i].dot(W)\n",
        "\n",
        "        # Prevent numerical instability by subtracting max score\n",
        "        scores -= np.max(scores)\n",
        "\n",
        "        # Compute the softmax probabilities\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "        # Compute the loss for the correct class\n",
        "        loss -= np.log(probs[y[i]])\n",
        "\n",
        "        # Compute the gradient for each class\n",
        "        for j in range(num_classes):\n",
        "            if j == y[i]:\n",
        "                dW[:, j] += (probs[j] - 1) * X[i]\n",
        "            else:\n",
        "                dW[:, j] += probs[j] * X[i]\n",
        "\n",
        "    # Average the loss and gradients over the number of training examples\n",
        "    loss /= num_train\n",
        "    dW /= num_train\n",
        "\n",
        "    # Add regularization to the loss and gradient\n",
        "    loss += 0.5 * alpha * np.sum(W * W)\n",
        "    dW += alpha * W\n",
        "\n",
        "    return loss, dW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9aa529",
      "metadata": {
        "id": "8b9aa529"
      },
      "source": [
        "### Without regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0eea225f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ac16981288c3c2f604ad8bab7defad33",
          "grade": true,
          "grade_id": "cell-878cebbdfa4e3f9b",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eea225f",
        "outputId": "c71d81e4-654d-48d1-d501-d342a2093bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical: -0.167171 analytic: -0.167171, relative error: 2.947539e-10\n",
            "numerical: -0.275190 analytic: -0.275190, relative error: 2.730880e-11\n",
            "numerical: 0.764030 analytic: 0.764030, relative error: 6.211300e-11\n",
            "numerical: -0.030729 analytic: -0.030729, relative error: 4.470522e-09\n",
            "numerical: -0.124547 analytic: -0.124547, relative error: 2.391774e-10\n",
            "numerical: 0.764030 analytic: 0.764030, relative error: 6.211300e-11\n",
            "numerical: 0.095839 analytic: 0.095839, relative error: 1.123248e-10\n",
            "numerical: 0.277176 analytic: 0.277176, relative error: 5.497121e-10\n",
            "numerical: -0.246447 analytic: -0.246447, relative error: 5.812440e-10\n",
            "numerical: -0.275190 analytic: -0.275190, relative error: 2.730880e-11\n",
            "numerical: 0.764030 analytic: 0.764030, relative error: 6.211300e-11\n",
            "numerical: 0.277176 analytic: 0.277176, relative error: 5.497121e-10\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_naive(W, X_train2, y_train2, 0.0)\n",
        "\n",
        "f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edea2f2",
      "metadata": {
        "id": "7edea2f2"
      },
      "source": [
        "### With regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "533001ff",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bd9a1e462d23666f55369d6b9d152117",
          "grade": true,
          "grade_id": "cell-ead29b6569263a0f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "533001ff",
        "outputId": "f567424e-4457-4777-d26f-b68f5a3af8e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical: 0.028966 analytic: 0.028966, relative error: 1.353426e-09\n",
            "numerical: -0.166984 analytic: -0.166984, relative error: 2.817379e-10\n",
            "numerical: 0.028966 analytic: 0.028966, relative error: 1.353426e-09\n",
            "numerical: -0.124735 analytic: -0.124735, relative error: 2.660126e-10\n",
            "numerical: 0.096013 analytic: 0.096013, relative error: 1.687716e-10\n",
            "numerical: -0.030942 analytic: -0.030942, relative error: 4.601240e-09\n",
            "numerical: -0.246217 analytic: -0.246217, relative error: 5.864328e-10\n",
            "numerical: 0.096013 analytic: 0.096013, relative error: 1.687716e-10\n",
            "numerical: -0.274926 analytic: -0.274926, relative error: 3.338375e-11\n",
            "numerical: 0.317158 analytic: 0.317158, relative error: 8.815231e-11\n",
            "numerical: -0.596998 analytic: -0.596998, relative error: 1.000548e-10\n",
            "numerical: -0.124735 analytic: -0.124735, relative error: 2.660126e-10\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_naive(W, X_train2, y_train2, 2)\n",
        "\n",
        "f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 2)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242d8a82-b477-4065-a0f5-91c9f4f91f86",
      "metadata": {
        "id": "242d8a82-b477-4065-a0f5-91c9f4f91f86"
      },
      "source": [
        "## Vectorized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c2593d82",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "48f52a4942e2b468f5775c5c7fbd2617",
          "grade": false,
          "grade_id": "cell-11e6980597b20334",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c2593d82"
      },
      "outputs": [],
      "source": [
        "def softmax_loss_vectorized(W, X, y, alpha, fit_intercept=False):\n",
        "    \"\"\"\n",
        "    Softmax loss function WITHOUT FOR LOOPS\n",
        "\n",
        "    Inputs:\n",
        "    - W: array of shape (D, C) containing weights\n",
        "    - X: array of shape (N, D) containing a minibatch of data\n",
        "    - y: array of shape (N,) containing training labels\n",
        "    - alpha: (float) regularization\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W;  same shape as W\n",
        "    \"\"\"\n",
        "    # Compute scores\n",
        "    scores = X.dot(W)\n",
        "    scores -= np.max(scores, axis=1, keepdims=True)  # For numerical stability\n",
        "\n",
        "    # Compute softmax probabilities\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute the loss\n",
        "    N = X.shape[0]\n",
        "    correct_log_probs = -np.log(probs[np.arange(N), y])\n",
        "    loss = np.sum(correct_log_probs) / N\n",
        "    loss += 0.5 * alpha * np.sum(W * W)\n",
        "\n",
        "    # Compute the gradient\n",
        "    dscores = probs\n",
        "    dscores[np.arange(N), y] -= 1\n",
        "    dscores /= N\n",
        "    dW = X.T.dot(dscores)\n",
        "    dW += alpha * W\n",
        "\n",
        "    return loss, dW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddea508b",
      "metadata": {
        "id": "ddea508b"
      },
      "source": [
        "### Without regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "78f759e2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "441d40ba642d3a505582eaeb87981127",
          "grade": true,
          "grade_id": "cell-0bc0fcf0bd3d3f13",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78f759e2",
        "outputId": "dd05e32b-6ae7-4265-d074-e66f1fbf880e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical: -0.275190 analytic: -0.275190, relative error: 4.748076e-11\n",
            "numerical: -0.275190 analytic: -0.275190, relative error: 4.748076e-11\n",
            "numerical: 0.317351 analytic: 0.317351, relative error: 2.460166e-12\n",
            "numerical: -0.275190 analytic: -0.275190, relative error: 4.748076e-11\n",
            "numerical: -0.030729 analytic: -0.030729, relative error: 4.109230e-09\n",
            "numerical: -0.275190 analytic: -0.275190, relative error: 4.748076e-11\n",
            "numerical: -0.124547 analytic: -0.124547, relative error: 1.946067e-10\n",
            "numerical: 0.764030 analytic: 0.764030, relative error: 6.211285e-11\n",
            "numerical: -0.246447 analytic: -0.246447, relative error: 5.136696e-10\n",
            "numerical: 0.317351 analytic: 0.317351, relative error: 2.460166e-12\n",
            "numerical: -0.246447 analytic: -0.246447, relative error: 5.136696e-10\n",
            "numerical: 0.764030 analytic: 0.764030, relative error: 6.211285e-11\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 0.0)\n",
        "\n",
        "f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c84f53",
      "metadata": {
        "id": "27c84f53"
      },
      "source": [
        "### With regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8bddf43a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6475ed3a14441d5488ecb46ec2522bbb",
          "grade": true,
          "grade_id": "cell-1fe0149d053dade0",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bddf43a",
        "outputId": "56755650-1494-4733-b270-8a644dee24ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical: -0.166984 analytic: -0.166984, relative error: 3.482261e-10\n",
            "numerical: -0.041942 analytic: -0.041942, relative error: 2.849529e-11\n",
            "numerical: 0.764016 analytic: 0.764016, relative error: 5.876659e-11\n",
            "numerical: 0.277011 analytic: 0.277011, relative error: 4.880493e-10\n",
            "numerical: 0.764016 analytic: 0.764016, relative error: 5.876659e-11\n",
            "numerical: 0.028966 analytic: 0.028966, relative error: 7.785096e-10\n",
            "numerical: -0.246217 analytic: -0.246217, relative error: 5.187952e-10\n",
            "numerical: -0.124735 analytic: -0.124735, relative error: 2.215092e-10\n",
            "numerical: 0.764016 analytic: 0.764016, relative error: 5.876659e-11\n",
            "numerical: -0.124735 analytic: -0.124735, relative error: 2.215092e-10\n",
            "numerical: -0.030942 analytic: -0.030942, relative error: 4.242436e-09\n",
            "numerical: -0.274926 analytic: -0.274926, relative error: 5.357502e-11\n"
          ]
        }
      ],
      "source": [
        "loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 2)\n",
        "\n",
        "f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 2)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab7e247a",
      "metadata": {
        "id": "ab7e247a"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9dff0bbc",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a0cedf5c70f5cf04cdd8b74702815dba",
          "grade": false,
          "grade_id": "cell-0f69bb891603b665",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "9dff0bbc"
      },
      "outputs": [],
      "source": [
        "class LinearModel():\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.W = None\n",
        "        self.fit_intercept = fit_intercept\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
        "        if self.fit_intercept:\n",
        "            # Add a bias term by appending a column of ones to X\n",
        "            X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "        N, d = X.shape\n",
        "        C = (np.max(y) + 1)  # Number of classes\n",
        "\n",
        "        if self.W is None:  # Initialize weights\n",
        "            self.W = 0.001 * np.random.randn(d, C)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            # Sample batch_size elements in X_batch and y_batch\n",
        "            indices = np.random.choice(N, batch_size, replace=True)\n",
        "            X_batch = X[indices]\n",
        "            y_batch = y[indices]\n",
        "\n",
        "            # Evaluate loss and gradient\n",
        "            loss, dW = self.loss(X_batch, y_batch, alpha)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # Perform parameter update\n",
        "            self.W -= learning_rate * dW\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print(f\"iteration {it} / {num_iters}: loss {loss}\")\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.fit_intercept:\n",
        "            # Add a bias term to X\n",
        "            X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "        # Compute class scores and select the class with the highest score\n",
        "        scores = X.dot(self.W)\n",
        "        y_pred = np.argmax(scores, axis=1)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch, reg):\n",
        "        raise NotImplementedError(\"Subclasses should implement this method!\")\n",
        "\n",
        "class MultinomialLogisticRegressor(LinearModel):\n",
        "    \"\"\" Softmax regression \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch, alpha):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch, alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - X: array of shape (N, D)\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: 1-dimensional array of length N, each element is an integer giving the predicted class\n",
        "        \"\"\"\n",
        "        return super().predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ce21aaa4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "56aacc6a9e4ede50cd934a81dfd9915f",
          "grade": true,
          "grade_id": "cell-8569aecb5759a819",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce21aaa4",
        "outputId": "6318408e-51ed-4d3a-ce04-d8ba2bca7da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 / 75000: loss 1.098905045952543\n",
            "iteration 100 / 75000: loss 0.9993766506973472\n",
            "iteration 200 / 75000: loss 0.9444479018014497\n",
            "iteration 300 / 75000: loss 0.8919579960560728\n",
            "iteration 400 / 75000: loss 0.8047147297387693\n",
            "iteration 500 / 75000: loss 0.8146247194914082\n",
            "iteration 600 / 75000: loss 0.7480013028772616\n",
            "iteration 700 / 75000: loss 0.7357646455203711\n",
            "iteration 800 / 75000: loss 0.6460431102789556\n",
            "iteration 900 / 75000: loss 0.6279178713420663\n",
            "iteration 1000 / 75000: loss 0.6366190434443029\n",
            "iteration 1100 / 75000: loss 0.6629978276182614\n",
            "iteration 1200 / 75000: loss 0.6747941807102598\n",
            "iteration 1300 / 75000: loss 0.5670818197962113\n",
            "iteration 1400 / 75000: loss 0.6136295943852457\n",
            "iteration 1500 / 75000: loss 0.5896559817655131\n",
            "iteration 1600 / 75000: loss 0.5744535458657958\n",
            "iteration 1700 / 75000: loss 0.5753446767524498\n",
            "iteration 1800 / 75000: loss 0.5591741887310802\n",
            "iteration 1900 / 75000: loss 0.5792989108627926\n",
            "iteration 2000 / 75000: loss 0.5063092605611257\n",
            "iteration 2100 / 75000: loss 0.5329343154709794\n",
            "iteration 2200 / 75000: loss 0.5327127643172316\n",
            "iteration 2300 / 75000: loss 0.43297090823127227\n",
            "iteration 2400 / 75000: loss 0.4297379369449862\n",
            "iteration 2500 / 75000: loss 0.4392969980972625\n",
            "iteration 2600 / 75000: loss 0.39099909572627956\n",
            "iteration 2700 / 75000: loss 0.400900889951162\n",
            "iteration 2800 / 75000: loss 0.44531148987626495\n",
            "iteration 2900 / 75000: loss 0.4402256188020122\n",
            "iteration 3000 / 75000: loss 0.4555458243020167\n",
            "iteration 3100 / 75000: loss 0.48018311024590077\n",
            "iteration 3200 / 75000: loss 0.4572011112229877\n",
            "iteration 3300 / 75000: loss 0.4236786207940452\n",
            "iteration 3400 / 75000: loss 0.5158163880170379\n",
            "iteration 3500 / 75000: loss 0.4940690235452237\n",
            "iteration 3600 / 75000: loss 0.5473258777727379\n",
            "iteration 3700 / 75000: loss 0.5212063779555676\n",
            "iteration 3800 / 75000: loss 0.4171542983227072\n",
            "iteration 3900 / 75000: loss 0.4625942463601851\n",
            "iteration 4000 / 75000: loss 0.477217155475755\n",
            "iteration 4100 / 75000: loss 0.5078860605999529\n",
            "iteration 4200 / 75000: loss 0.390445705787586\n",
            "iteration 4300 / 75000: loss 0.40966638183519744\n",
            "iteration 4400 / 75000: loss 0.4039667295690165\n",
            "iteration 4500 / 75000: loss 0.4300942793131389\n",
            "iteration 4600 / 75000: loss 0.4347109319833578\n",
            "iteration 4700 / 75000: loss 0.3897508423176755\n",
            "iteration 4800 / 75000: loss 0.4175300974643097\n",
            "iteration 4900 / 75000: loss 0.4390200629758613\n",
            "iteration 5000 / 75000: loss 0.5019625661717044\n",
            "iteration 5100 / 75000: loss 0.4317792854687804\n",
            "iteration 5200 / 75000: loss 0.39109964559199545\n",
            "iteration 5300 / 75000: loss 0.4764564337368131\n",
            "iteration 5400 / 75000: loss 0.40971659085827494\n",
            "iteration 5500 / 75000: loss 0.4990641779803874\n",
            "iteration 5600 / 75000: loss 0.5007672483843333\n",
            "iteration 5700 / 75000: loss 0.36397708672217044\n",
            "iteration 5800 / 75000: loss 0.42506405401662495\n",
            "iteration 5900 / 75000: loss 0.5271469104190126\n",
            "iteration 6000 / 75000: loss 0.354974336194517\n",
            "iteration 6100 / 75000: loss 0.3884556016290731\n",
            "iteration 6200 / 75000: loss 0.44597720949340436\n",
            "iteration 6300 / 75000: loss 0.35610304290858946\n",
            "iteration 6400 / 75000: loss 0.37776310467072405\n",
            "iteration 6500 / 75000: loss 0.47175123732298324\n",
            "iteration 6600 / 75000: loss 0.3968755234619243\n",
            "iteration 6700 / 75000: loss 0.32374989531275367\n",
            "iteration 6800 / 75000: loss 0.3082373743574378\n",
            "iteration 6900 / 75000: loss 0.2965019941600888\n",
            "iteration 7000 / 75000: loss 0.3648512401765699\n",
            "iteration 7100 / 75000: loss 0.4751827110473529\n",
            "iteration 7200 / 75000: loss 0.4493007630472104\n",
            "iteration 7300 / 75000: loss 0.35252167630955555\n",
            "iteration 7400 / 75000: loss 0.4014550307819062\n",
            "iteration 7500 / 75000: loss 0.48902118172843784\n",
            "iteration 7600 / 75000: loss 0.40809719187513643\n",
            "iteration 7700 / 75000: loss 0.3401235260045199\n",
            "iteration 7800 / 75000: loss 0.3651963795247408\n",
            "iteration 7900 / 75000: loss 0.44869626724722933\n",
            "iteration 8000 / 75000: loss 0.3340308704340982\n",
            "iteration 8100 / 75000: loss 0.3420713398059976\n",
            "iteration 8200 / 75000: loss 0.47913027034765204\n",
            "iteration 8300 / 75000: loss 0.44020365299122\n",
            "iteration 8400 / 75000: loss 0.39817398369511686\n",
            "iteration 8500 / 75000: loss 0.4447909967855944\n",
            "iteration 8600 / 75000: loss 0.38547167615801536\n",
            "iteration 8700 / 75000: loss 0.427665515377735\n",
            "iteration 8800 / 75000: loss 0.4261930859676933\n",
            "iteration 8900 / 75000: loss 0.4076141540261764\n",
            "iteration 9000 / 75000: loss 0.3965225578352737\n",
            "iteration 9100 / 75000: loss 0.34955200539594555\n",
            "iteration 9200 / 75000: loss 0.37884648406594323\n",
            "iteration 9300 / 75000: loss 0.3894257856933926\n",
            "iteration 9400 / 75000: loss 0.42097093559758664\n",
            "iteration 9500 / 75000: loss 0.34340134359697594\n",
            "iteration 9600 / 75000: loss 0.40575631123995637\n",
            "iteration 9700 / 75000: loss 0.3736233913380231\n",
            "iteration 9800 / 75000: loss 0.40099108945781264\n",
            "iteration 9900 / 75000: loss 0.2960971295510759\n",
            "iteration 10000 / 75000: loss 0.4729396416142122\n",
            "iteration 10100 / 75000: loss 0.3424981746713089\n",
            "iteration 10200 / 75000: loss 0.3479350627366359\n",
            "iteration 10300 / 75000: loss 0.48213250945408126\n",
            "iteration 10400 / 75000: loss 0.4159566090428041\n",
            "iteration 10500 / 75000: loss 0.36694854934571286\n",
            "iteration 10600 / 75000: loss 0.29166693377903724\n",
            "iteration 10700 / 75000: loss 0.32808031412200556\n",
            "iteration 10800 / 75000: loss 0.29367855880038585\n",
            "iteration 10900 / 75000: loss 0.36723760293801055\n",
            "iteration 11000 / 75000: loss 0.32355641762398735\n",
            "iteration 11100 / 75000: loss 0.43362917216643393\n",
            "iteration 11200 / 75000: loss 0.32438576325415\n",
            "iteration 11300 / 75000: loss 0.4702043875339393\n",
            "iteration 11400 / 75000: loss 0.31976266482362903\n",
            "iteration 11500 / 75000: loss 0.40628970747818827\n",
            "iteration 11600 / 75000: loss 0.42405257870284263\n",
            "iteration 11700 / 75000: loss 0.3937815190474041\n",
            "iteration 11800 / 75000: loss 0.33600765200627447\n",
            "iteration 11900 / 75000: loss 0.3120583160670051\n",
            "iteration 12000 / 75000: loss 0.3701734378638514\n",
            "iteration 12100 / 75000: loss 0.4275470712776591\n",
            "iteration 12200 / 75000: loss 0.4497791416050142\n",
            "iteration 12300 / 75000: loss 0.354645950427183\n",
            "iteration 12400 / 75000: loss 0.33723615655632844\n",
            "iteration 12500 / 75000: loss 0.32563864728206343\n",
            "iteration 12600 / 75000: loss 0.3771231128932698\n",
            "iteration 12700 / 75000: loss 0.49959238633772507\n",
            "iteration 12800 / 75000: loss 0.41227443971651845\n",
            "iteration 12900 / 75000: loss 0.38459488764417815\n",
            "iteration 13000 / 75000: loss 0.4059350147161809\n",
            "iteration 13100 / 75000: loss 0.40589952507623217\n",
            "iteration 13200 / 75000: loss 0.43029362845265573\n",
            "iteration 13300 / 75000: loss 0.39129773440002835\n",
            "iteration 13400 / 75000: loss 0.24716432575264508\n",
            "iteration 13500 / 75000: loss 0.3788528974970684\n",
            "iteration 13600 / 75000: loss 0.37689419427258974\n",
            "iteration 13700 / 75000: loss 0.37213685281383235\n",
            "iteration 13800 / 75000: loss 0.5020025279362099\n",
            "iteration 13900 / 75000: loss 0.37175614947516\n",
            "iteration 14000 / 75000: loss 0.35579309422016986\n",
            "iteration 14100 / 75000: loss 0.3530668106974646\n",
            "iteration 14200 / 75000: loss 0.3026456817870057\n",
            "iteration 14300 / 75000: loss 0.377114739431638\n",
            "iteration 14400 / 75000: loss 0.34903393739475985\n",
            "iteration 14500 / 75000: loss 0.39539466539837437\n",
            "iteration 14600 / 75000: loss 0.34847017039782324\n",
            "iteration 14700 / 75000: loss 0.39193358255700317\n",
            "iteration 14800 / 75000: loss 0.3517188028348877\n",
            "iteration 14900 / 75000: loss 0.34559778695868903\n",
            "iteration 15000 / 75000: loss 0.40518563256024875\n",
            "iteration 15100 / 75000: loss 0.3168420350628892\n",
            "iteration 15200 / 75000: loss 0.4280304203617141\n",
            "iteration 15300 / 75000: loss 0.3195747593761809\n",
            "iteration 15400 / 75000: loss 0.4567387068232571\n",
            "iteration 15500 / 75000: loss 0.3517221960854887\n",
            "iteration 15600 / 75000: loss 0.328950330042474\n",
            "iteration 15700 / 75000: loss 0.41828020143607003\n",
            "iteration 15800 / 75000: loss 0.34388627229507485\n",
            "iteration 15900 / 75000: loss 0.352404926502509\n",
            "iteration 16000 / 75000: loss 0.36412022683293066\n",
            "iteration 16100 / 75000: loss 0.3733803171062763\n",
            "iteration 16200 / 75000: loss 0.40660693178501756\n",
            "iteration 16300 / 75000: loss 0.369309951657139\n",
            "iteration 16400 / 75000: loss 0.4525572336411271\n",
            "iteration 16500 / 75000: loss 0.3107197534450822\n",
            "iteration 16600 / 75000: loss 0.40994246382565436\n",
            "iteration 16700 / 75000: loss 0.38180168729749664\n",
            "iteration 16800 / 75000: loss 0.41670945217124433\n",
            "iteration 16900 / 75000: loss 0.4660181340771975\n",
            "iteration 17000 / 75000: loss 0.3724610986707252\n",
            "iteration 17100 / 75000: loss 0.35069426099955475\n",
            "iteration 17200 / 75000: loss 0.35570992095449083\n",
            "iteration 17300 / 75000: loss 0.28786538828075925\n",
            "iteration 17400 / 75000: loss 0.46825392958893985\n",
            "iteration 17500 / 75000: loss 0.38434325215776755\n",
            "iteration 17600 / 75000: loss 0.3293991960453041\n",
            "iteration 17700 / 75000: loss 0.2806404732935768\n",
            "iteration 17800 / 75000: loss 0.4253403833718543\n",
            "iteration 17900 / 75000: loss 0.27387327211959456\n",
            "iteration 18000 / 75000: loss 0.33381671116838907\n",
            "iteration 18100 / 75000: loss 0.2904490364847929\n",
            "iteration 18200 / 75000: loss 0.3556843435463816\n",
            "iteration 18300 / 75000: loss 0.35130996411866255\n",
            "iteration 18400 / 75000: loss 0.37963344883918804\n",
            "iteration 18500 / 75000: loss 0.4724970999978755\n",
            "iteration 18600 / 75000: loss 0.3884274784709906\n",
            "iteration 18700 / 75000: loss 0.34467684441351765\n",
            "iteration 18800 / 75000: loss 0.498330140279755\n",
            "iteration 18900 / 75000: loss 0.382143484718111\n",
            "iteration 19000 / 75000: loss 0.35202159503006253\n",
            "iteration 19100 / 75000: loss 0.292699475089409\n",
            "iteration 19200 / 75000: loss 0.3769983031602836\n",
            "iteration 19300 / 75000: loss 0.4309406980913417\n",
            "iteration 19400 / 75000: loss 0.3048743027638228\n",
            "iteration 19500 / 75000: loss 0.28990209893014296\n",
            "iteration 19600 / 75000: loss 0.30748831075669164\n",
            "iteration 19700 / 75000: loss 0.2530733606922956\n",
            "iteration 19800 / 75000: loss 0.5133120459956375\n",
            "iteration 19900 / 75000: loss 0.34075037265284314\n",
            "iteration 20000 / 75000: loss 0.23191563938043694\n",
            "iteration 20100 / 75000: loss 0.436591207130251\n",
            "iteration 20200 / 75000: loss 0.3050356869834273\n",
            "iteration 20300 / 75000: loss 0.3449212698727623\n",
            "iteration 20400 / 75000: loss 0.4384077257127039\n",
            "iteration 20500 / 75000: loss 0.4424700245179811\n",
            "iteration 20600 / 75000: loss 0.40618621774580826\n",
            "iteration 20700 / 75000: loss 0.2651348024668492\n",
            "iteration 20800 / 75000: loss 0.3726008053966849\n",
            "iteration 20900 / 75000: loss 0.38116022725337145\n",
            "iteration 21000 / 75000: loss 0.28586199264086953\n",
            "iteration 21100 / 75000: loss 0.3015027630524285\n",
            "iteration 21200 / 75000: loss 0.36780628600195925\n",
            "iteration 21300 / 75000: loss 0.38932457695685535\n",
            "iteration 21400 / 75000: loss 0.3511713822625771\n",
            "iteration 21500 / 75000: loss 0.34131794374966373\n",
            "iteration 21600 / 75000: loss 0.32441411623756744\n",
            "iteration 21700 / 75000: loss 0.3207228951820159\n",
            "iteration 21800 / 75000: loss 0.408401429466007\n",
            "iteration 21900 / 75000: loss 0.4344046065392635\n",
            "iteration 22000 / 75000: loss 0.31650121149410626\n",
            "iteration 22100 / 75000: loss 0.34150608707984487\n",
            "iteration 22200 / 75000: loss 0.3250715235046903\n",
            "iteration 22300 / 75000: loss 0.4293527903861938\n",
            "iteration 22400 / 75000: loss 0.4316934028187734\n",
            "iteration 22500 / 75000: loss 0.29153337998680573\n",
            "iteration 22600 / 75000: loss 0.2768516115217871\n",
            "iteration 22700 / 75000: loss 0.3834336428706216\n",
            "iteration 22800 / 75000: loss 0.3309365982160006\n",
            "iteration 22900 / 75000: loss 0.3449979905448128\n",
            "iteration 23000 / 75000: loss 0.4257783777836871\n",
            "iteration 23100 / 75000: loss 0.31028505493582853\n",
            "iteration 23200 / 75000: loss 0.29701831135385426\n",
            "iteration 23300 / 75000: loss 0.3376344891170555\n",
            "iteration 23400 / 75000: loss 0.3053331518361728\n",
            "iteration 23500 / 75000: loss 0.30609692643483166\n",
            "iteration 23600 / 75000: loss 0.4171874064347858\n",
            "iteration 23700 / 75000: loss 0.48265505524225016\n",
            "iteration 23800 / 75000: loss 0.23535582475083533\n",
            "iteration 23900 / 75000: loss 0.28218108451948276\n",
            "iteration 24000 / 75000: loss 0.4237330608318358\n",
            "iteration 24100 / 75000: loss 0.3483671379995038\n",
            "iteration 24200 / 75000: loss 0.37325219758626205\n",
            "iteration 24300 / 75000: loss 0.26793606800869396\n",
            "iteration 24400 / 75000: loss 0.26123112770009294\n",
            "iteration 24500 / 75000: loss 0.3606575283077903\n",
            "iteration 24600 / 75000: loss 0.293297543033482\n",
            "iteration 24700 / 75000: loss 0.4672096095801872\n",
            "iteration 24800 / 75000: loss 0.34856558167474083\n",
            "iteration 24900 / 75000: loss 0.28887492896162825\n",
            "iteration 25000 / 75000: loss 0.33509687459423637\n",
            "iteration 25100 / 75000: loss 0.26303474367006685\n",
            "iteration 25200 / 75000: loss 0.3224484294063302\n",
            "iteration 25300 / 75000: loss 0.29337101967380186\n",
            "iteration 25400 / 75000: loss 0.4565907941048532\n",
            "iteration 25500 / 75000: loss 0.3779153238588862\n",
            "iteration 25600 / 75000: loss 0.4497820568879147\n",
            "iteration 25700 / 75000: loss 0.25906379380632105\n",
            "iteration 25800 / 75000: loss 0.3087142989588638\n",
            "iteration 25900 / 75000: loss 0.2718959441500103\n",
            "iteration 26000 / 75000: loss 0.3155845159920301\n",
            "iteration 26100 / 75000: loss 0.3708175542632116\n",
            "iteration 26200 / 75000: loss 0.3343858870864519\n",
            "iteration 26300 / 75000: loss 0.3918954893245767\n",
            "iteration 26400 / 75000: loss 0.4203526179215203\n",
            "iteration 26500 / 75000: loss 0.4543373947806898\n",
            "iteration 26600 / 75000: loss 0.29301163235851824\n",
            "iteration 26700 / 75000: loss 0.28694555295638646\n",
            "iteration 26800 / 75000: loss 0.38525077604463254\n",
            "iteration 26900 / 75000: loss 0.3037204903574715\n",
            "iteration 27000 / 75000: loss 0.2947418339293821\n",
            "iteration 27100 / 75000: loss 0.2799241222425896\n",
            "iteration 27200 / 75000: loss 0.4787143944204564\n",
            "iteration 27300 / 75000: loss 0.3904098122601991\n",
            "iteration 27400 / 75000: loss 0.33176461207498353\n",
            "iteration 27500 / 75000: loss 0.359921661148092\n",
            "iteration 27600 / 75000: loss 0.3777320285814109\n",
            "iteration 27700 / 75000: loss 0.29394206774689174\n",
            "iteration 27800 / 75000: loss 0.35900316650070946\n",
            "iteration 27900 / 75000: loss 0.2637955980203559\n",
            "iteration 28000 / 75000: loss 0.30477419612630363\n",
            "iteration 28100 / 75000: loss 0.2802047322414719\n",
            "iteration 28200 / 75000: loss 0.34035657520113294\n",
            "iteration 28300 / 75000: loss 0.30561982362299417\n",
            "iteration 28400 / 75000: loss 0.3453970583448086\n",
            "iteration 28500 / 75000: loss 0.3426536012341678\n",
            "iteration 28600 / 75000: loss 0.212786017762023\n",
            "iteration 28700 / 75000: loss 0.32485943025069663\n",
            "iteration 28800 / 75000: loss 0.34263202381584856\n",
            "iteration 28900 / 75000: loss 0.3431640323268632\n",
            "iteration 29000 / 75000: loss 0.2358695798065864\n",
            "iteration 29100 / 75000: loss 0.44762065145276714\n",
            "iteration 29200 / 75000: loss 0.3710974633006906\n",
            "iteration 29300 / 75000: loss 0.3694376207278841\n",
            "iteration 29400 / 75000: loss 0.36468349964106916\n",
            "iteration 29500 / 75000: loss 0.33160264890889485\n",
            "iteration 29600 / 75000: loss 0.37814690123372013\n",
            "iteration 29700 / 75000: loss 0.3247946155320867\n",
            "iteration 29800 / 75000: loss 0.3026237900443718\n",
            "iteration 29900 / 75000: loss 0.3568457261072945\n",
            "iteration 30000 / 75000: loss 0.35390765618394393\n",
            "iteration 30100 / 75000: loss 0.3772852143034688\n",
            "iteration 30200 / 75000: loss 0.43526202793430474\n",
            "iteration 30300 / 75000: loss 0.3271817899183085\n",
            "iteration 30400 / 75000: loss 0.2769930154145134\n",
            "iteration 30500 / 75000: loss 0.35100820682305633\n",
            "iteration 30600 / 75000: loss 0.4014379504366414\n",
            "iteration 30700 / 75000: loss 0.3105820223393697\n",
            "iteration 30800 / 75000: loss 0.3207910708613416\n",
            "iteration 30900 / 75000: loss 0.28208875792653915\n",
            "iteration 31000 / 75000: loss 0.32686932220184284\n",
            "iteration 31100 / 75000: loss 0.39239105551550524\n",
            "iteration 31200 / 75000: loss 0.3540509247392375\n",
            "iteration 31300 / 75000: loss 0.3994646389065979\n",
            "iteration 31400 / 75000: loss 0.3946885061725359\n",
            "iteration 31500 / 75000: loss 0.3393439523613908\n",
            "iteration 31600 / 75000: loss 0.288930098634314\n",
            "iteration 31700 / 75000: loss 0.3816350069104121\n",
            "iteration 31800 / 75000: loss 0.4135734532673522\n",
            "iteration 31900 / 75000: loss 0.4112027528967175\n",
            "iteration 32000 / 75000: loss 0.3070521535034594\n",
            "iteration 32100 / 75000: loss 0.2592519984970356\n",
            "iteration 32200 / 75000: loss 0.41424627746080844\n",
            "iteration 32300 / 75000: loss 0.38154228468470985\n",
            "iteration 32400 / 75000: loss 0.3367679154852435\n",
            "iteration 32500 / 75000: loss 0.3308284692076918\n",
            "iteration 32600 / 75000: loss 0.3989391359366962\n",
            "iteration 32700 / 75000: loss 0.41423079045341415\n",
            "iteration 32800 / 75000: loss 0.417841647739419\n",
            "iteration 32900 / 75000: loss 0.31893892106398314\n",
            "iteration 33000 / 75000: loss 0.358496642374118\n",
            "iteration 33100 / 75000: loss 0.2961105257957714\n",
            "iteration 33200 / 75000: loss 0.3399186110349381\n",
            "iteration 33300 / 75000: loss 0.3886034237755404\n",
            "iteration 33400 / 75000: loss 0.3264095421587797\n",
            "iteration 33500 / 75000: loss 0.30114483730996394\n",
            "iteration 33600 / 75000: loss 0.36866191316835983\n",
            "iteration 33700 / 75000: loss 0.2789564754039015\n",
            "iteration 33800 / 75000: loss 0.4018868505459807\n",
            "iteration 33900 / 75000: loss 0.3437998066539577\n",
            "iteration 34000 / 75000: loss 0.4106303328248726\n",
            "iteration 34100 / 75000: loss 0.4224898182505943\n",
            "iteration 34200 / 75000: loss 0.39967610758708416\n",
            "iteration 34300 / 75000: loss 0.3305290336163831\n",
            "iteration 34400 / 75000: loss 0.2954416217999607\n",
            "iteration 34500 / 75000: loss 0.2557596507104708\n",
            "iteration 34600 / 75000: loss 0.3342559941063511\n",
            "iteration 34700 / 75000: loss 0.5478114082692597\n",
            "iteration 34800 / 75000: loss 0.3267153186019725\n",
            "iteration 34900 / 75000: loss 0.44093482725302235\n",
            "iteration 35000 / 75000: loss 0.2705986090656877\n",
            "iteration 35100 / 75000: loss 0.3167037285521353\n",
            "iteration 35200 / 75000: loss 0.3577232970683614\n",
            "iteration 35300 / 75000: loss 0.36473078923339197\n",
            "iteration 35400 / 75000: loss 0.44269276922393785\n",
            "iteration 35500 / 75000: loss 0.37058320372892734\n",
            "iteration 35600 / 75000: loss 0.40612448422735725\n",
            "iteration 35700 / 75000: loss 0.38125548818140287\n",
            "iteration 35800 / 75000: loss 0.32556657646362364\n",
            "iteration 35900 / 75000: loss 0.3226212297942297\n",
            "iteration 36000 / 75000: loss 0.4900067081794718\n",
            "iteration 36100 / 75000: loss 0.20995701991173915\n",
            "iteration 36200 / 75000: loss 0.3693039932499631\n",
            "iteration 36300 / 75000: loss 0.3189615860514724\n",
            "iteration 36400 / 75000: loss 0.23985498314127063\n",
            "iteration 36500 / 75000: loss 0.36749074592868775\n",
            "iteration 36600 / 75000: loss 0.33668627362532133\n",
            "iteration 36700 / 75000: loss 0.39116510021510315\n",
            "iteration 36800 / 75000: loss 0.3669295067779649\n",
            "iteration 36900 / 75000: loss 0.38421929384065473\n",
            "iteration 37000 / 75000: loss 0.31865432405901767\n",
            "iteration 37100 / 75000: loss 0.4134269187264828\n",
            "iteration 37200 / 75000: loss 0.24424209192435037\n",
            "iteration 37300 / 75000: loss 0.3542149455256702\n",
            "iteration 37400 / 75000: loss 0.3173170154766979\n",
            "iteration 37500 / 75000: loss 0.29673607725980744\n",
            "iteration 37600 / 75000: loss 0.30538049153884245\n",
            "iteration 37700 / 75000: loss 0.2597680610329211\n",
            "iteration 37800 / 75000: loss 0.2909746841707663\n",
            "iteration 37900 / 75000: loss 0.2711572277202393\n",
            "iteration 38000 / 75000: loss 0.3853466020547859\n",
            "iteration 38100 / 75000: loss 0.3246107119562348\n",
            "iteration 38200 / 75000: loss 0.2750700805055035\n",
            "iteration 38300 / 75000: loss 0.4260150610409571\n",
            "iteration 38400 / 75000: loss 0.22054137250122363\n",
            "iteration 38500 / 75000: loss 0.3050359868211846\n",
            "iteration 38600 / 75000: loss 0.2697602255987444\n",
            "iteration 38700 / 75000: loss 0.43688343973447885\n",
            "iteration 38800 / 75000: loss 0.27490102542317674\n",
            "iteration 38900 / 75000: loss 0.23346313533211027\n",
            "iteration 39000 / 75000: loss 0.2474935468784859\n",
            "iteration 39100 / 75000: loss 0.4063977163001452\n",
            "iteration 39200 / 75000: loss 0.32091082678684413\n",
            "iteration 39300 / 75000: loss 0.34161743385496646\n",
            "iteration 39400 / 75000: loss 0.4157951226574006\n",
            "iteration 39500 / 75000: loss 0.3014423028513915\n",
            "iteration 39600 / 75000: loss 0.3077028414717867\n",
            "iteration 39700 / 75000: loss 0.3366729899133143\n",
            "iteration 39800 / 75000: loss 0.38417565198495074\n",
            "iteration 39900 / 75000: loss 0.4434857601012834\n",
            "iteration 40000 / 75000: loss 0.34885054816005934\n",
            "iteration 40100 / 75000: loss 0.3950700235122868\n",
            "iteration 40200 / 75000: loss 0.3565951630214393\n",
            "iteration 40300 / 75000: loss 0.3445316410479541\n",
            "iteration 40400 / 75000: loss 0.40935503889716185\n",
            "iteration 40500 / 75000: loss 0.2790576189197401\n",
            "iteration 40600 / 75000: loss 0.25143459138399393\n",
            "iteration 40700 / 75000: loss 0.33388198022241056\n",
            "iteration 40800 / 75000: loss 0.33175052085423185\n",
            "iteration 40900 / 75000: loss 0.3223284743095566\n",
            "iteration 41000 / 75000: loss 0.34536942804989057\n",
            "iteration 41100 / 75000: loss 0.345708305524422\n",
            "iteration 41200 / 75000: loss 0.3079134158022163\n",
            "iteration 41300 / 75000: loss 0.2945673935932765\n",
            "iteration 41400 / 75000: loss 0.27974002059191894\n",
            "iteration 41500 / 75000: loss 0.31971382113995184\n",
            "iteration 41600 / 75000: loss 0.25873591512786176\n",
            "iteration 41700 / 75000: loss 0.26695936011958465\n",
            "iteration 41800 / 75000: loss 0.31625398267758487\n",
            "iteration 41900 / 75000: loss 0.4139596604960002\n",
            "iteration 42000 / 75000: loss 0.3281706746563209\n",
            "iteration 42100 / 75000: loss 0.3924642456245824\n",
            "iteration 42200 / 75000: loss 0.38196137846133943\n",
            "iteration 42300 / 75000: loss 0.31790466779052184\n",
            "iteration 42400 / 75000: loss 0.3246667540147018\n",
            "iteration 42500 / 75000: loss 0.32129328462515355\n",
            "iteration 42600 / 75000: loss 0.39722720959614277\n",
            "iteration 42700 / 75000: loss 0.3273490463113443\n",
            "iteration 42800 / 75000: loss 0.2715712994767834\n",
            "iteration 42900 / 75000: loss 0.3499302771193424\n",
            "iteration 43000 / 75000: loss 0.3214052782944171\n",
            "iteration 43100 / 75000: loss 0.44257424118806354\n",
            "iteration 43200 / 75000: loss 0.41742367904039523\n",
            "iteration 43300 / 75000: loss 0.4348309392044174\n",
            "iteration 43400 / 75000: loss 0.22363444507015595\n",
            "iteration 43500 / 75000: loss 0.2668418307769111\n",
            "iteration 43600 / 75000: loss 0.2774385637190031\n",
            "iteration 43700 / 75000: loss 0.3724311483162781\n",
            "iteration 43800 / 75000: loss 0.3246386277442528\n",
            "iteration 43900 / 75000: loss 0.3208428574316371\n",
            "iteration 44000 / 75000: loss 0.3694430854774011\n",
            "iteration 44100 / 75000: loss 0.4791003310760841\n",
            "iteration 44200 / 75000: loss 0.3874789578335744\n",
            "iteration 44300 / 75000: loss 0.3456234098943934\n",
            "iteration 44400 / 75000: loss 0.3642352850947087\n",
            "iteration 44500 / 75000: loss 0.3882853861480139\n",
            "iteration 44600 / 75000: loss 0.3505006927622939\n",
            "iteration 44700 / 75000: loss 0.33008882081055535\n",
            "iteration 44800 / 75000: loss 0.3208087996174295\n",
            "iteration 44900 / 75000: loss 0.16966579592539988\n",
            "iteration 45000 / 75000: loss 0.2669300133109326\n",
            "iteration 45100 / 75000: loss 0.20644754382583733\n",
            "iteration 45200 / 75000: loss 0.4017972444624834\n",
            "iteration 45300 / 75000: loss 0.2499177079894903\n",
            "iteration 45400 / 75000: loss 0.2931059691862543\n",
            "iteration 45500 / 75000: loss 0.29058154544911735\n",
            "iteration 45600 / 75000: loss 0.3162489546674959\n",
            "iteration 45700 / 75000: loss 0.41205216700848757\n",
            "iteration 45800 / 75000: loss 0.3836054836831938\n",
            "iteration 45900 / 75000: loss 0.37232622060868437\n",
            "iteration 46000 / 75000: loss 0.3108696502299439\n",
            "iteration 46100 / 75000: loss 0.3258854680685276\n",
            "iteration 46200 / 75000: loss 0.5618756457862604\n",
            "iteration 46300 / 75000: loss 0.2926663751613904\n",
            "iteration 46400 / 75000: loss 0.4009454152696401\n",
            "iteration 46500 / 75000: loss 0.325284436902074\n",
            "iteration 46600 / 75000: loss 0.35045799840317093\n",
            "iteration 46700 / 75000: loss 0.253716411733139\n",
            "iteration 46800 / 75000: loss 0.4175568370369246\n",
            "iteration 46900 / 75000: loss 0.3684737526041007\n",
            "iteration 47000 / 75000: loss 0.2895270394041555\n",
            "iteration 47100 / 75000: loss 0.35147612749049495\n",
            "iteration 47200 / 75000: loss 0.3077561156253529\n",
            "iteration 47300 / 75000: loss 0.30251949837839914\n",
            "iteration 47400 / 75000: loss 0.2506520428910312\n",
            "iteration 47500 / 75000: loss 0.39508471097479475\n",
            "iteration 47600 / 75000: loss 0.30203192656653177\n",
            "iteration 47700 / 75000: loss 0.2774186986677798\n",
            "iteration 47800 / 75000: loss 0.245356980573324\n",
            "iteration 47900 / 75000: loss 0.3804579571344445\n",
            "iteration 48000 / 75000: loss 0.4025019501519456\n",
            "iteration 48100 / 75000: loss 0.378270123522027\n",
            "iteration 48200 / 75000: loss 0.2632437966968789\n",
            "iteration 48300 / 75000: loss 0.413953141144934\n",
            "iteration 48400 / 75000: loss 0.3397469728284407\n",
            "iteration 48500 / 75000: loss 0.27672407048133707\n",
            "iteration 48600 / 75000: loss 0.3652684785297361\n",
            "iteration 48700 / 75000: loss 0.29739481492358766\n",
            "iteration 48800 / 75000: loss 0.22844328050824217\n",
            "iteration 48900 / 75000: loss 0.4327575129063398\n",
            "iteration 49000 / 75000: loss 0.3627772607244161\n",
            "iteration 49100 / 75000: loss 0.39212452255933156\n",
            "iteration 49200 / 75000: loss 0.26774131617619157\n",
            "iteration 49300 / 75000: loss 0.2892902407650766\n",
            "iteration 49400 / 75000: loss 0.315607105391029\n",
            "iteration 49500 / 75000: loss 0.3015822241199721\n",
            "iteration 49600 / 75000: loss 0.2967222704927144\n",
            "iteration 49700 / 75000: loss 0.36182647930086076\n",
            "iteration 49800 / 75000: loss 0.29945950550935263\n",
            "iteration 49900 / 75000: loss 0.40823818563649433\n",
            "iteration 50000 / 75000: loss 0.38352603837908433\n",
            "iteration 50100 / 75000: loss 0.31990307778742666\n",
            "iteration 50200 / 75000: loss 0.3588480434875615\n",
            "iteration 50300 / 75000: loss 0.34196130570617367\n",
            "iteration 50400 / 75000: loss 0.4584978235892236\n",
            "iteration 50500 / 75000: loss 0.33615610010574476\n",
            "iteration 50600 / 75000: loss 0.314102893535278\n",
            "iteration 50700 / 75000: loss 0.36519254452183825\n",
            "iteration 50800 / 75000: loss 0.33996997164107867\n",
            "iteration 50900 / 75000: loss 0.3741722545302233\n",
            "iteration 51000 / 75000: loss 0.275969951083368\n",
            "iteration 51100 / 75000: loss 0.2552462633487568\n",
            "iteration 51200 / 75000: loss 0.2517001652620928\n",
            "iteration 51300 / 75000: loss 0.32212658356732965\n",
            "iteration 51400 / 75000: loss 0.4171941464568204\n",
            "iteration 51500 / 75000: loss 0.3679401918376622\n",
            "iteration 51600 / 75000: loss 0.37677481269943114\n",
            "iteration 51700 / 75000: loss 0.3198174607416056\n",
            "iteration 51800 / 75000: loss 0.3032086642019103\n",
            "iteration 51900 / 75000: loss 0.46999395057981547\n",
            "iteration 52000 / 75000: loss 0.41805493960732326\n",
            "iteration 52100 / 75000: loss 0.40152041808165906\n",
            "iteration 52200 / 75000: loss 0.4302571911261406\n",
            "iteration 52300 / 75000: loss 0.4014169770358028\n",
            "iteration 52400 / 75000: loss 0.30534485059787564\n",
            "iteration 52500 / 75000: loss 0.2880183854304248\n",
            "iteration 52600 / 75000: loss 0.34482572952206736\n",
            "iteration 52700 / 75000: loss 0.517054477142237\n",
            "iteration 52800 / 75000: loss 0.4123285932552511\n",
            "iteration 52900 / 75000: loss 0.293148572613032\n",
            "iteration 53000 / 75000: loss 0.3012726897884157\n",
            "iteration 53100 / 75000: loss 0.40838640625580425\n",
            "iteration 53200 / 75000: loss 0.3435487970246742\n",
            "iteration 53300 / 75000: loss 0.3244830493436782\n",
            "iteration 53400 / 75000: loss 0.3951324409740717\n",
            "iteration 53500 / 75000: loss 0.2870425301267526\n",
            "iteration 53600 / 75000: loss 0.29233786273083096\n",
            "iteration 53700 / 75000: loss 0.36758919058042644\n",
            "iteration 53800 / 75000: loss 0.42100963426719745\n",
            "iteration 53900 / 75000: loss 0.27677862112862095\n",
            "iteration 54000 / 75000: loss 0.3799416328680363\n",
            "iteration 54100 / 75000: loss 0.34254039683143345\n",
            "iteration 54200 / 75000: loss 0.4696440621208313\n",
            "iteration 54300 / 75000: loss 0.29560649852536103\n",
            "iteration 54400 / 75000: loss 0.34643372854247667\n",
            "iteration 54500 / 75000: loss 0.318624072911976\n",
            "iteration 54600 / 75000: loss 0.2479429456973492\n",
            "iteration 54700 / 75000: loss 0.34735366222719355\n",
            "iteration 54800 / 75000: loss 0.30605697521887976\n",
            "iteration 54900 / 75000: loss 0.3332378116163164\n",
            "iteration 55000 / 75000: loss 0.28574967760299685\n",
            "iteration 55100 / 75000: loss 0.38342430457373294\n",
            "iteration 55200 / 75000: loss 0.4917089008854225\n",
            "iteration 55300 / 75000: loss 0.48361464176651486\n",
            "iteration 55400 / 75000: loss 0.3439127846590404\n",
            "iteration 55500 / 75000: loss 0.33770043046801124\n",
            "iteration 55600 / 75000: loss 0.3827542136692383\n",
            "iteration 55700 / 75000: loss 0.39056172485832086\n",
            "iteration 55800 / 75000: loss 0.33160010210330554\n",
            "iteration 55900 / 75000: loss 0.3295191712214791\n",
            "iteration 56000 / 75000: loss 0.3608933504053998\n",
            "iteration 56100 / 75000: loss 0.3303507037829949\n",
            "iteration 56200 / 75000: loss 0.3192722963274167\n",
            "iteration 56300 / 75000: loss 0.26520972200934767\n",
            "iteration 56400 / 75000: loss 0.27774282458183036\n",
            "iteration 56500 / 75000: loss 0.34124440980709314\n",
            "iteration 56600 / 75000: loss 0.4192973115853641\n",
            "iteration 56700 / 75000: loss 0.2567661399571852\n",
            "iteration 56800 / 75000: loss 0.4099081537328256\n",
            "iteration 56900 / 75000: loss 0.3721738016867975\n",
            "iteration 57000 / 75000: loss 0.3004287934293409\n",
            "iteration 57100 / 75000: loss 0.3521198066152149\n",
            "iteration 57200 / 75000: loss 0.39798492943858715\n",
            "iteration 57300 / 75000: loss 0.29760556029774576\n",
            "iteration 57400 / 75000: loss 0.25022845456928283\n",
            "iteration 57500 / 75000: loss 0.42561576787475447\n",
            "iteration 57600 / 75000: loss 0.30969883813123283\n",
            "iteration 57700 / 75000: loss 0.3018308163731868\n",
            "iteration 57800 / 75000: loss 0.5316676600887291\n",
            "iteration 57900 / 75000: loss 0.391837668542293\n",
            "iteration 58000 / 75000: loss 0.3752122673968098\n",
            "iteration 58100 / 75000: loss 0.21684300623050046\n",
            "iteration 58200 / 75000: loss 0.25079435224336066\n",
            "iteration 58300 / 75000: loss 0.322748211294967\n",
            "iteration 58400 / 75000: loss 0.3054582924819931\n",
            "iteration 58500 / 75000: loss 0.34670248011274896\n",
            "iteration 58600 / 75000: loss 0.3269313401199276\n",
            "iteration 58700 / 75000: loss 0.2805225399256269\n",
            "iteration 58800 / 75000: loss 0.3615251350726962\n",
            "iteration 58900 / 75000: loss 0.395834288450572\n",
            "iteration 59000 / 75000: loss 0.33823093252410646\n",
            "iteration 59100 / 75000: loss 0.37575478046249544\n",
            "iteration 59200 / 75000: loss 0.3061855304831954\n",
            "iteration 59300 / 75000: loss 0.3684874318740311\n",
            "iteration 59400 / 75000: loss 0.22319248119057564\n",
            "iteration 59500 / 75000: loss 0.3840803312764798\n",
            "iteration 59600 / 75000: loss 0.29634056920831225\n",
            "iteration 59700 / 75000: loss 0.30637847564912385\n",
            "iteration 59800 / 75000: loss 0.24240750392177857\n",
            "iteration 59900 / 75000: loss 0.2969800614222068\n",
            "iteration 60000 / 75000: loss 0.40426894667925617\n",
            "iteration 60100 / 75000: loss 0.29711354306689486\n",
            "iteration 60200 / 75000: loss 0.35190883786763105\n",
            "iteration 60300 / 75000: loss 0.32549432057246297\n",
            "iteration 60400 / 75000: loss 0.23669885528477064\n",
            "iteration 60500 / 75000: loss 0.33000185946245597\n",
            "iteration 60600 / 75000: loss 0.38034689701885926\n",
            "iteration 60700 / 75000: loss 0.2801777505392682\n",
            "iteration 60800 / 75000: loss 0.2901203202113525\n",
            "iteration 60900 / 75000: loss 0.3565142116483576\n",
            "iteration 61000 / 75000: loss 0.4546950889289799\n",
            "iteration 61100 / 75000: loss 0.3773289574862845\n",
            "iteration 61200 / 75000: loss 0.2977279878467475\n",
            "iteration 61300 / 75000: loss 0.21332594913761666\n",
            "iteration 61400 / 75000: loss 0.3022471786631697\n",
            "iteration 61500 / 75000: loss 0.2980556181027975\n",
            "iteration 61600 / 75000: loss 0.4202622396636457\n",
            "iteration 61700 / 75000: loss 0.3589739499089486\n",
            "iteration 61800 / 75000: loss 0.37576581571505535\n",
            "iteration 61900 / 75000: loss 0.34503549027896513\n",
            "iteration 62000 / 75000: loss 0.31323963151700085\n",
            "iteration 62100 / 75000: loss 0.3298558773853276\n",
            "iteration 62200 / 75000: loss 0.38156909614032153\n",
            "iteration 62300 / 75000: loss 0.3008642034616467\n",
            "iteration 62400 / 75000: loss 0.30214184106172415\n",
            "iteration 62500 / 75000: loss 0.252111373156672\n",
            "iteration 62600 / 75000: loss 0.2817999066674017\n",
            "iteration 62700 / 75000: loss 0.3169579259218779\n",
            "iteration 62800 / 75000: loss 0.351954180350235\n",
            "iteration 62900 / 75000: loss 0.26705250268296565\n",
            "iteration 63000 / 75000: loss 0.3993327984416593\n",
            "iteration 63100 / 75000: loss 0.24637159460237495\n",
            "iteration 63200 / 75000: loss 0.29222978243470965\n",
            "iteration 63300 / 75000: loss 0.2991859332926171\n",
            "iteration 63400 / 75000: loss 0.30408954457474546\n",
            "iteration 63500 / 75000: loss 0.30695858576811563\n",
            "iteration 63600 / 75000: loss 0.30827233046292624\n",
            "iteration 63700 / 75000: loss 0.2481339059482103\n",
            "iteration 63800 / 75000: loss 0.1668064895091143\n",
            "iteration 63900 / 75000: loss 0.2623980048173462\n",
            "iteration 64000 / 75000: loss 0.20873546173992863\n",
            "iteration 64100 / 75000: loss 0.31041954059873045\n",
            "iteration 64200 / 75000: loss 0.3373368723276147\n",
            "iteration 64300 / 75000: loss 0.4139931320568889\n",
            "iteration 64400 / 75000: loss 0.43595402469967076\n",
            "iteration 64500 / 75000: loss 0.19057739802194357\n",
            "iteration 64600 / 75000: loss 0.3802032100999883\n",
            "iteration 64700 / 75000: loss 0.2704850093045402\n",
            "iteration 64800 / 75000: loss 0.28974532443846507\n",
            "iteration 64900 / 75000: loss 0.2641827156798844\n",
            "iteration 65000 / 75000: loss 0.385158701102473\n",
            "iteration 65100 / 75000: loss 0.3131121490824301\n",
            "iteration 65200 / 75000: loss 0.3916564438356559\n",
            "iteration 65300 / 75000: loss 0.3198966253852694\n",
            "iteration 65400 / 75000: loss 0.31081131285975505\n",
            "iteration 65500 / 75000: loss 0.2907844093000277\n",
            "iteration 65600 / 75000: loss 0.2879463592726149\n",
            "iteration 65700 / 75000: loss 0.33172800645104783\n",
            "iteration 65800 / 75000: loss 0.3966431854588799\n",
            "iteration 65900 / 75000: loss 0.40181947303951415\n",
            "iteration 66000 / 75000: loss 0.3179807110108\n",
            "iteration 66100 / 75000: loss 0.3270146921464125\n",
            "iteration 66200 / 75000: loss 0.40096376743858253\n",
            "iteration 66300 / 75000: loss 0.31889940586280274\n",
            "iteration 66400 / 75000: loss 0.3653003653792729\n",
            "iteration 66500 / 75000: loss 0.38426414182867996\n",
            "iteration 66600 / 75000: loss 0.36436899525861727\n",
            "iteration 66700 / 75000: loss 0.23201856967594903\n",
            "iteration 66800 / 75000: loss 0.49718143990796826\n",
            "iteration 66900 / 75000: loss 0.2901256960316224\n",
            "iteration 67000 / 75000: loss 0.2889372504318328\n",
            "iteration 67100 / 75000: loss 0.3559794073066054\n",
            "iteration 67200 / 75000: loss 0.3424158432178985\n",
            "iteration 67300 / 75000: loss 0.3218057824369763\n",
            "iteration 67400 / 75000: loss 0.2762217975817741\n",
            "iteration 67500 / 75000: loss 0.3409621910124004\n",
            "iteration 67600 / 75000: loss 0.2978837751395834\n",
            "iteration 67700 / 75000: loss 0.3092779993534771\n",
            "iteration 67800 / 75000: loss 0.3162347082082003\n",
            "iteration 67900 / 75000: loss 0.2830661586792228\n",
            "iteration 68000 / 75000: loss 0.23459873519948532\n",
            "iteration 68100 / 75000: loss 0.2928035494388024\n",
            "iteration 68200 / 75000: loss 0.26767542292040275\n",
            "iteration 68300 / 75000: loss 0.4516164978322522\n",
            "iteration 68400 / 75000: loss 0.3465127493056541\n",
            "iteration 68500 / 75000: loss 0.3570814937680352\n",
            "iteration 68600 / 75000: loss 0.3706910249760781\n",
            "iteration 68700 / 75000: loss 0.26149149588656384\n",
            "iteration 68800 / 75000: loss 0.3488151997149547\n",
            "iteration 68900 / 75000: loss 0.1978375319489148\n",
            "iteration 69000 / 75000: loss 0.291143912547606\n",
            "iteration 69100 / 75000: loss 0.33475232041157976\n",
            "iteration 69200 / 75000: loss 0.3286898818799164\n",
            "iteration 69300 / 75000: loss 0.2924701615386781\n",
            "iteration 69400 / 75000: loss 0.3797154874477258\n",
            "iteration 69500 / 75000: loss 0.3689593192316532\n",
            "iteration 69600 / 75000: loss 0.25484967300526706\n",
            "iteration 69700 / 75000: loss 0.34740232894875744\n",
            "iteration 69800 / 75000: loss 0.3619294815896809\n",
            "iteration 69900 / 75000: loss 0.35279632120960525\n",
            "iteration 70000 / 75000: loss 0.3970005289944033\n",
            "iteration 70100 / 75000: loss 0.35573765430538207\n",
            "iteration 70200 / 75000: loss 0.29707527157162067\n",
            "iteration 70300 / 75000: loss 0.3569100238679481\n",
            "iteration 70400 / 75000: loss 0.285854956906004\n",
            "iteration 70500 / 75000: loss 0.38052138487885645\n",
            "iteration 70600 / 75000: loss 0.31506263705658283\n",
            "iteration 70700 / 75000: loss 0.3670210577732991\n",
            "iteration 70800 / 75000: loss 0.33717250307780033\n",
            "iteration 70900 / 75000: loss 0.33117246223479097\n",
            "iteration 71000 / 75000: loss 0.3343407375728811\n",
            "iteration 71100 / 75000: loss 0.25762271219356125\n",
            "iteration 71200 / 75000: loss 0.4606959695408281\n",
            "iteration 71300 / 75000: loss 0.32485490893399205\n",
            "iteration 71400 / 75000: loss 0.1941171908851057\n",
            "iteration 71500 / 75000: loss 0.19428148333268663\n",
            "iteration 71600 / 75000: loss 0.37711642187099736\n",
            "iteration 71700 / 75000: loss 0.297775977717586\n",
            "iteration 71800 / 75000: loss 0.27630303292457\n",
            "iteration 71900 / 75000: loss 0.3107591692618465\n",
            "iteration 72000 / 75000: loss 0.35029745435306814\n",
            "iteration 72100 / 75000: loss 0.2726845292803239\n",
            "iteration 72200 / 75000: loss 0.26357662773262963\n",
            "iteration 72300 / 75000: loss 0.4608868125944655\n",
            "iteration 72400 / 75000: loss 0.3413670994952532\n",
            "iteration 72500 / 75000: loss 0.3586168462986139\n",
            "iteration 72600 / 75000: loss 0.3449256162540546\n",
            "iteration 72700 / 75000: loss 0.246989818461004\n",
            "iteration 72800 / 75000: loss 0.31323506064767537\n",
            "iteration 72900 / 75000: loss 0.27727847340952155\n",
            "iteration 73000 / 75000: loss 0.33204139116775444\n",
            "iteration 73100 / 75000: loss 0.3181943334885053\n",
            "iteration 73200 / 75000: loss 0.3920244296898139\n",
            "iteration 73300 / 75000: loss 0.3132432505888196\n",
            "iteration 73400 / 75000: loss 0.36373582851351227\n",
            "iteration 73500 / 75000: loss 0.35156720729422947\n",
            "iteration 73600 / 75000: loss 0.29462554812101577\n",
            "iteration 73700 / 75000: loss 0.4205703298036205\n",
            "iteration 73800 / 75000: loss 0.31021285582881586\n",
            "iteration 73900 / 75000: loss 0.3198277845074894\n",
            "iteration 74000 / 75000: loss 0.2300067055474025\n",
            "iteration 74100 / 75000: loss 0.419679061675737\n",
            "iteration 74200 / 75000: loss 0.38291092266475935\n",
            "iteration 74300 / 75000: loss 0.3366123479023876\n",
            "iteration 74400 / 75000: loss 0.3399983359153243\n",
            "iteration 74500 / 75000: loss 0.2695197278266779\n",
            "iteration 74600 / 75000: loss 0.3076278483224626\n",
            "iteration 74700 / 75000: loss 0.31617052155427683\n",
            "iteration 74800 / 75000: loss 0.3661738230807962\n",
            "iteration 74900 / 75000: loss 0.23017840837856274\n",
            "Accuracy scikit-learn: 0.86\n",
            "Accuracy gradient descent model : 0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train2 = scaler.fit_transform(X_train2)\n",
        "\n",
        "sk_model = LogisticRegression(fit_intercept=False)\n",
        "sk_model.fit(X_train2, y_train2)\n",
        "sk_pred = sk_model.predict(X_train2)\n",
        "sk_accuracy = accuracy_score(y_train2, sk_pred)\n",
        "\n",
        "model = MultinomialLogisticRegressor(fit_intercept=False)\n",
        "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
        "pred = model.predict(X_train2)\n",
        "model_accuracy = accuracy_score(y_train2, pred)\n",
        "\n",
        "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
        "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
        "assert sk_accuracy - model_accuracy < 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b2c6a8ad",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40a4732103eaf77860c941e0897de01c",
          "grade": true,
          "grade_id": "cell-30e12569fdfb269c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2c6a8ad",
        "outputId": "96623648-4519-4f17-825f-71e75d40a93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 / 75000: loss 1.0985757836893404\n",
            "iteration 100 / 75000: loss 1.0093602324852786\n",
            "iteration 200 / 75000: loss 0.9338753981716212\n",
            "iteration 300 / 75000: loss 0.84486353219474\n",
            "iteration 400 / 75000: loss 0.8414073992547166\n",
            "iteration 500 / 75000: loss 0.7782176326457502\n",
            "iteration 600 / 75000: loss 0.7134887965879351\n",
            "iteration 700 / 75000: loss 0.7424957883871117\n",
            "iteration 800 / 75000: loss 0.6885052527809503\n",
            "iteration 900 / 75000: loss 0.722829136861787\n",
            "iteration 1000 / 75000: loss 0.6640191007131884\n",
            "iteration 1100 / 75000: loss 0.598943813231269\n",
            "iteration 1200 / 75000: loss 0.6336991038609436\n",
            "iteration 1300 / 75000: loss 0.5860456481287339\n",
            "iteration 1400 / 75000: loss 0.5676641408368388\n",
            "iteration 1500 / 75000: loss 0.6173317941175052\n",
            "iteration 1600 / 75000: loss 0.5548884254851565\n",
            "iteration 1700 / 75000: loss 0.5672416686411131\n",
            "iteration 1800 / 75000: loss 0.5163700005940827\n",
            "iteration 1900 / 75000: loss 0.5772274573770106\n",
            "iteration 2000 / 75000: loss 0.5295104976828622\n",
            "iteration 2100 / 75000: loss 0.45795275486286735\n",
            "iteration 2200 / 75000: loss 0.5354711323634349\n",
            "iteration 2300 / 75000: loss 0.49650858827193656\n",
            "iteration 2400 / 75000: loss 0.45197308140243647\n",
            "iteration 2500 / 75000: loss 0.4283431769916009\n",
            "iteration 2600 / 75000: loss 0.4516331181894181\n",
            "iteration 2700 / 75000: loss 0.5189711238385291\n",
            "iteration 2800 / 75000: loss 0.46393568977233607\n",
            "iteration 2900 / 75000: loss 0.4759884030618132\n",
            "iteration 3000 / 75000: loss 0.5106405290106055\n",
            "iteration 3100 / 75000: loss 0.43922956930348733\n",
            "iteration 3200 / 75000: loss 0.4037418746791279\n",
            "iteration 3300 / 75000: loss 0.4912093319399495\n",
            "iteration 3400 / 75000: loss 0.40443124140190956\n",
            "iteration 3500 / 75000: loss 0.49266786018307385\n",
            "iteration 3600 / 75000: loss 0.5168707701104919\n",
            "iteration 3700 / 75000: loss 0.46236021293777896\n",
            "iteration 3800 / 75000: loss 0.4336622936714829\n",
            "iteration 3900 / 75000: loss 0.45860055323524385\n",
            "iteration 4000 / 75000: loss 0.4579195225763617\n",
            "iteration 4100 / 75000: loss 0.43014324972829243\n",
            "iteration 4200 / 75000: loss 0.4141700008827773\n",
            "iteration 4300 / 75000: loss 0.42958063864968526\n",
            "iteration 4400 / 75000: loss 0.4290787552924785\n",
            "iteration 4500 / 75000: loss 0.49212974104411467\n",
            "iteration 4600 / 75000: loss 0.3867404681327353\n",
            "iteration 4700 / 75000: loss 0.4446076995877337\n",
            "iteration 4800 / 75000: loss 0.4876506268480804\n",
            "iteration 4900 / 75000: loss 0.4383765961472321\n",
            "iteration 5000 / 75000: loss 0.4325729156689367\n",
            "iteration 5100 / 75000: loss 0.43157289553847256\n",
            "iteration 5200 / 75000: loss 0.38389294248163075\n",
            "iteration 5300 / 75000: loss 0.3824563048033316\n",
            "iteration 5400 / 75000: loss 0.3553895329578407\n",
            "iteration 5500 / 75000: loss 0.4172116920709485\n",
            "iteration 5600 / 75000: loss 0.3748784214379327\n",
            "iteration 5700 / 75000: loss 0.4909732173986582\n",
            "iteration 5800 / 75000: loss 0.42416666249403595\n",
            "iteration 5900 / 75000: loss 0.4161930027699209\n",
            "iteration 6000 / 75000: loss 0.34501783998241775\n",
            "iteration 6100 / 75000: loss 0.4210849134086159\n",
            "iteration 6200 / 75000: loss 0.3786814493897017\n",
            "iteration 6300 / 75000: loss 0.43943572681345466\n",
            "iteration 6400 / 75000: loss 0.41887223481232067\n",
            "iteration 6500 / 75000: loss 0.40230521915309425\n",
            "iteration 6600 / 75000: loss 0.4378607394457694\n",
            "iteration 6700 / 75000: loss 0.34875966248758566\n",
            "iteration 6800 / 75000: loss 0.3297083792853348\n",
            "iteration 6900 / 75000: loss 0.3896987445750454\n",
            "iteration 7000 / 75000: loss 0.3697696136875025\n",
            "iteration 7100 / 75000: loss 0.3852984732040062\n",
            "iteration 7200 / 75000: loss 0.37884780853975564\n",
            "iteration 7300 / 75000: loss 0.4517375859827565\n",
            "iteration 7400 / 75000: loss 0.38781825280054183\n",
            "iteration 7500 / 75000: loss 0.31622984686729627\n",
            "iteration 7600 / 75000: loss 0.3717972554452299\n",
            "iteration 7700 / 75000: loss 0.3333971400647251\n",
            "iteration 7800 / 75000: loss 0.3713692227231773\n",
            "iteration 7900 / 75000: loss 0.34207734974666826\n",
            "iteration 8000 / 75000: loss 0.35236696981361826\n",
            "iteration 8100 / 75000: loss 0.2652803609546581\n",
            "iteration 8200 / 75000: loss 0.3427466917024525\n",
            "iteration 8300 / 75000: loss 0.3772775260601284\n",
            "iteration 8400 / 75000: loss 0.42133760908234064\n",
            "iteration 8500 / 75000: loss 0.43270061033304913\n",
            "iteration 8600 / 75000: loss 0.30269232477251595\n",
            "iteration 8700 / 75000: loss 0.330453220604414\n",
            "iteration 8800 / 75000: loss 0.3223495474553318\n",
            "iteration 8900 / 75000: loss 0.35538827825138924\n",
            "iteration 9000 / 75000: loss 0.2593992021533983\n",
            "iteration 9100 / 75000: loss 0.32025804480630676\n",
            "iteration 9200 / 75000: loss 0.37495886925936367\n",
            "iteration 9300 / 75000: loss 0.33319637217948894\n",
            "iteration 9400 / 75000: loss 0.33045674114364537\n",
            "iteration 9500 / 75000: loss 0.41277421546095744\n",
            "iteration 9600 / 75000: loss 0.3642368380791693\n",
            "iteration 9700 / 75000: loss 0.3065851425567325\n",
            "iteration 9800 / 75000: loss 0.2912856158624694\n",
            "iteration 9900 / 75000: loss 0.34955124564494455\n",
            "iteration 10000 / 75000: loss 0.37914576114114384\n",
            "iteration 10100 / 75000: loss 0.3606107079413166\n",
            "iteration 10200 / 75000: loss 0.3691557197409294\n",
            "iteration 10300 / 75000: loss 0.37633481403382657\n",
            "iteration 10400 / 75000: loss 0.32375929627880945\n",
            "iteration 10500 / 75000: loss 0.289416867011997\n",
            "iteration 10600 / 75000: loss 0.3320545045568108\n",
            "iteration 10700 / 75000: loss 0.3515205450500497\n",
            "iteration 10800 / 75000: loss 0.37204373351217324\n",
            "iteration 10900 / 75000: loss 0.43370750663891344\n",
            "iteration 11000 / 75000: loss 0.3031746012292292\n",
            "iteration 11100 / 75000: loss 0.35953853213032017\n",
            "iteration 11200 / 75000: loss 0.34408172828928724\n",
            "iteration 11300 / 75000: loss 0.3480572369603364\n",
            "iteration 11400 / 75000: loss 0.3001291502881622\n",
            "iteration 11500 / 75000: loss 0.41623764582493\n",
            "iteration 11600 / 75000: loss 0.35726732312024617\n",
            "iteration 11700 / 75000: loss 0.2463057168765244\n",
            "iteration 11800 / 75000: loss 0.30817930868677346\n",
            "iteration 11900 / 75000: loss 0.36425560365473797\n",
            "iteration 12000 / 75000: loss 0.3839913420367287\n",
            "iteration 12100 / 75000: loss 0.26358078281196407\n",
            "iteration 12200 / 75000: loss 0.3259038859680398\n",
            "iteration 12300 / 75000: loss 0.27119725749639373\n",
            "iteration 12400 / 75000: loss 0.29054131667042404\n",
            "iteration 12500 / 75000: loss 0.33060069223209276\n",
            "iteration 12600 / 75000: loss 0.3236593030677165\n",
            "iteration 12700 / 75000: loss 0.2552302340259521\n",
            "iteration 12800 / 75000: loss 0.26866191650912374\n",
            "iteration 12900 / 75000: loss 0.36334130285605204\n",
            "iteration 13000 / 75000: loss 0.29956580793763776\n",
            "iteration 13100 / 75000: loss 0.34690860821470326\n",
            "iteration 13200 / 75000: loss 0.2874440104736379\n",
            "iteration 13300 / 75000: loss 0.30853616022048014\n",
            "iteration 13400 / 75000: loss 0.3375958868973177\n",
            "iteration 13500 / 75000: loss 0.2821806225257165\n",
            "iteration 13600 / 75000: loss 0.35949393758089027\n",
            "iteration 13700 / 75000: loss 0.3032856678027417\n",
            "iteration 13800 / 75000: loss 0.2860855213640754\n",
            "iteration 13900 / 75000: loss 0.26309259348111774\n",
            "iteration 14000 / 75000: loss 0.3012904359490728\n",
            "iteration 14100 / 75000: loss 0.260537121793799\n",
            "iteration 14200 / 75000: loss 0.27214961828529466\n",
            "iteration 14300 / 75000: loss 0.2751524312733117\n",
            "iteration 14400 / 75000: loss 0.31253336103995855\n",
            "iteration 14500 / 75000: loss 0.3511207490357614\n",
            "iteration 14600 / 75000: loss 0.3099136123072841\n",
            "iteration 14700 / 75000: loss 0.28240722576091626\n",
            "iteration 14800 / 75000: loss 0.2557342933329991\n",
            "iteration 14900 / 75000: loss 0.3324626453168785\n",
            "iteration 15000 / 75000: loss 0.32940102439886576\n",
            "iteration 15100 / 75000: loss 0.28577948276500675\n",
            "iteration 15200 / 75000: loss 0.31232112152979047\n",
            "iteration 15300 / 75000: loss 0.26996283781461305\n",
            "iteration 15400 / 75000: loss 0.2442115245557266\n",
            "iteration 15500 / 75000: loss 0.28114282553085257\n",
            "iteration 15600 / 75000: loss 0.25919826851849453\n",
            "iteration 15700 / 75000: loss 0.2340114385930832\n",
            "iteration 15800 / 75000: loss 0.2784526041741967\n",
            "iteration 15900 / 75000: loss 0.29888099307191673\n",
            "iteration 16000 / 75000: loss 0.22383872843365038\n",
            "iteration 16100 / 75000: loss 0.284365286521507\n",
            "iteration 16200 / 75000: loss 0.295988739221779\n",
            "iteration 16300 / 75000: loss 0.28214271447528294\n",
            "iteration 16400 / 75000: loss 0.2680701955846031\n",
            "iteration 16500 / 75000: loss 0.29420451368702716\n",
            "iteration 16600 / 75000: loss 0.24297253011547387\n",
            "iteration 16700 / 75000: loss 0.3418895502865837\n",
            "iteration 16800 / 75000: loss 0.22550007750161694\n",
            "iteration 16900 / 75000: loss 0.2848108968963118\n",
            "iteration 17000 / 75000: loss 0.27196410003063076\n",
            "iteration 17100 / 75000: loss 0.19038708841114144\n",
            "iteration 17200 / 75000: loss 0.24206210389352062\n",
            "iteration 17300 / 75000: loss 0.2661569249022008\n",
            "iteration 17400 / 75000: loss 0.3044704031476894\n",
            "iteration 17500 / 75000: loss 0.34323634942914816\n",
            "iteration 17600 / 75000: loss 0.2933966592517946\n",
            "iteration 17700 / 75000: loss 0.2796977466940976\n",
            "iteration 17800 / 75000: loss 0.2532629120854205\n",
            "iteration 17900 / 75000: loss 0.28226944852526137\n",
            "iteration 18000 / 75000: loss 0.29144958569036955\n",
            "iteration 18100 / 75000: loss 0.21151696625633842\n",
            "iteration 18200 / 75000: loss 0.237013137587957\n",
            "iteration 18300 / 75000: loss 0.291552424708316\n",
            "iteration 18400 / 75000: loss 0.2574346662774772\n",
            "iteration 18500 / 75000: loss 0.22348158868134457\n",
            "iteration 18600 / 75000: loss 0.3076261394100325\n",
            "iteration 18700 / 75000: loss 0.288573349789802\n",
            "iteration 18800 / 75000: loss 0.274164978228438\n",
            "iteration 18900 / 75000: loss 0.2645148883951134\n",
            "iteration 19000 / 75000: loss 0.24278754617601506\n",
            "iteration 19100 / 75000: loss 0.2806010134309407\n",
            "iteration 19200 / 75000: loss 0.3028433350438903\n",
            "iteration 19300 / 75000: loss 0.2188263642418961\n",
            "iteration 19400 / 75000: loss 0.2651913366139079\n",
            "iteration 19500 / 75000: loss 0.2756391428382142\n",
            "iteration 19600 / 75000: loss 0.2833962575816278\n",
            "iteration 19700 / 75000: loss 0.24326349923086737\n",
            "iteration 19800 / 75000: loss 0.25070661769686015\n",
            "iteration 19900 / 75000: loss 0.26849986679967147\n",
            "iteration 20000 / 75000: loss 0.30368095935798534\n",
            "iteration 20100 / 75000: loss 0.3067853368902203\n",
            "iteration 20200 / 75000: loss 0.26362656729023837\n",
            "iteration 20300 / 75000: loss 0.2850674909761576\n",
            "iteration 20400 / 75000: loss 0.22702263966208416\n",
            "iteration 20500 / 75000: loss 0.2899080898705436\n",
            "iteration 20600 / 75000: loss 0.26487956290553183\n",
            "iteration 20700 / 75000: loss 0.23574021169761955\n",
            "iteration 20800 / 75000: loss 0.32356535929071034\n",
            "iteration 20900 / 75000: loss 0.25532076829354916\n",
            "iteration 21000 / 75000: loss 0.21351628691740543\n",
            "iteration 21100 / 75000: loss 0.24656032994544558\n",
            "iteration 21200 / 75000: loss 0.2908710203296817\n",
            "iteration 21300 / 75000: loss 0.2595765325022612\n",
            "iteration 21400 / 75000: loss 0.24729098602791325\n",
            "iteration 21500 / 75000: loss 0.298258274425947\n",
            "iteration 21600 / 75000: loss 0.31316674686956386\n",
            "iteration 21700 / 75000: loss 0.20246565364481006\n",
            "iteration 21800 / 75000: loss 0.2088633711439717\n",
            "iteration 21900 / 75000: loss 0.2551066423979956\n",
            "iteration 22000 / 75000: loss 0.22249422788578477\n",
            "iteration 22100 / 75000: loss 0.2972549866459832\n",
            "iteration 22200 / 75000: loss 0.2274292286338836\n",
            "iteration 22300 / 75000: loss 0.2793116750337308\n",
            "iteration 22400 / 75000: loss 0.23892644234496868\n",
            "iteration 22500 / 75000: loss 0.21970688807429561\n",
            "iteration 22600 / 75000: loss 0.22431878051408327\n",
            "iteration 22700 / 75000: loss 0.28637308624111557\n",
            "iteration 22800 / 75000: loss 0.23301160491881262\n",
            "iteration 22900 / 75000: loss 0.22311514363125395\n",
            "iteration 23000 / 75000: loss 0.23016081126108334\n",
            "iteration 23100 / 75000: loss 0.233582111361975\n",
            "iteration 23200 / 75000: loss 0.29837277616788604\n",
            "iteration 23300 / 75000: loss 0.23688087136634922\n",
            "iteration 23400 / 75000: loss 0.30959656380543643\n",
            "iteration 23500 / 75000: loss 0.27148662969407567\n",
            "iteration 23600 / 75000: loss 0.24269718122819356\n",
            "iteration 23700 / 75000: loss 0.27242811397991706\n",
            "iteration 23800 / 75000: loss 0.3036799945199518\n",
            "iteration 23900 / 75000: loss 0.2837404901569658\n",
            "iteration 24000 / 75000: loss 0.22232179894323942\n",
            "iteration 24100 / 75000: loss 0.18935887228034115\n",
            "iteration 24200 / 75000: loss 0.22233598617296482\n",
            "iteration 24300 / 75000: loss 0.2470320570555265\n",
            "iteration 24400 / 75000: loss 0.2874830802853644\n",
            "iteration 24500 / 75000: loss 0.21344611538033623\n",
            "iteration 24600 / 75000: loss 0.22618856114173855\n",
            "iteration 24700 / 75000: loss 0.2403339541970691\n",
            "iteration 24800 / 75000: loss 0.2062140801637422\n",
            "iteration 24900 / 75000: loss 0.24751456771337693\n",
            "iteration 25000 / 75000: loss 0.21188068900259435\n",
            "iteration 25100 / 75000: loss 0.25663643444738216\n",
            "iteration 25200 / 75000: loss 0.20148554172187366\n",
            "iteration 25300 / 75000: loss 0.2537925697365282\n",
            "iteration 25400 / 75000: loss 0.2549182303760502\n",
            "iteration 25500 / 75000: loss 0.22843266074584728\n",
            "iteration 25600 / 75000: loss 0.2595946774786779\n",
            "iteration 25700 / 75000: loss 0.28803907580234633\n",
            "iteration 25800 / 75000: loss 0.17768918911925266\n",
            "iteration 25900 / 75000: loss 0.1518455514785613\n",
            "iteration 26000 / 75000: loss 0.2407317382391845\n",
            "iteration 26100 / 75000: loss 0.28039303527294046\n",
            "iteration 26200 / 75000: loss 0.22739976191790773\n",
            "iteration 26300 / 75000: loss 0.2000556615988378\n",
            "iteration 26400 / 75000: loss 0.23325724220613747\n",
            "iteration 26500 / 75000: loss 0.29274698380046427\n",
            "iteration 26600 / 75000: loss 0.22140363303098926\n",
            "iteration 26700 / 75000: loss 0.22087132293656572\n",
            "iteration 26800 / 75000: loss 0.24321153983616867\n",
            "iteration 26900 / 75000: loss 0.226149713213872\n",
            "iteration 27000 / 75000: loss 0.21346200746123478\n",
            "iteration 27100 / 75000: loss 0.25112951729329325\n",
            "iteration 27200 / 75000: loss 0.16307646165902348\n",
            "iteration 27300 / 75000: loss 0.22715717981731676\n",
            "iteration 27400 / 75000: loss 0.2286683543306006\n",
            "iteration 27500 / 75000: loss 0.21555434983218438\n",
            "iteration 27600 / 75000: loss 0.26992017967746634\n",
            "iteration 27700 / 75000: loss 0.2242064500981515\n",
            "iteration 27800 / 75000: loss 0.2743447163655458\n",
            "iteration 27900 / 75000: loss 0.23126334689330574\n",
            "iteration 28000 / 75000: loss 0.21976351996321702\n",
            "iteration 28100 / 75000: loss 0.31294909839231577\n",
            "iteration 28200 / 75000: loss 0.20950865825401727\n",
            "iteration 28300 / 75000: loss 0.29067454822146827\n",
            "iteration 28400 / 75000: loss 0.2241813692178231\n",
            "iteration 28500 / 75000: loss 0.21455880862922702\n",
            "iteration 28600 / 75000: loss 0.2073880589741769\n",
            "iteration 28700 / 75000: loss 0.227936694926523\n",
            "iteration 28800 / 75000: loss 0.24886062971858297\n",
            "iteration 28900 / 75000: loss 0.28309234630167734\n",
            "iteration 29000 / 75000: loss 0.21454948551675396\n",
            "iteration 29100 / 75000: loss 0.23803450436485674\n",
            "iteration 29200 / 75000: loss 0.22583030293160766\n",
            "iteration 29300 / 75000: loss 0.24079344278836878\n",
            "iteration 29400 / 75000: loss 0.2078755924596538\n",
            "iteration 29500 / 75000: loss 0.18953949614637428\n",
            "iteration 29600 / 75000: loss 0.19437964955355502\n",
            "iteration 29700 / 75000: loss 0.17709595348288237\n",
            "iteration 29800 / 75000: loss 0.23589589584366963\n",
            "iteration 29900 / 75000: loss 0.21728080043451603\n",
            "iteration 30000 / 75000: loss 0.26491280199696454\n",
            "iteration 30100 / 75000: loss 0.20624121470417262\n",
            "iteration 30200 / 75000: loss 0.23115673333806713\n",
            "iteration 30300 / 75000: loss 0.18560041536140576\n",
            "iteration 30400 / 75000: loss 0.1972426157789453\n",
            "iteration 30500 / 75000: loss 0.23578583856745683\n",
            "iteration 30600 / 75000: loss 0.22355062958451372\n",
            "iteration 30700 / 75000: loss 0.17996034933568933\n",
            "iteration 30800 / 75000: loss 0.24311011704085633\n",
            "iteration 30900 / 75000: loss 0.20780061488884055\n",
            "iteration 31000 / 75000: loss 0.16658969719198805\n",
            "iteration 31100 / 75000: loss 0.2307322751929231\n",
            "iteration 31200 / 75000: loss 0.2477323718738226\n",
            "iteration 31300 / 75000: loss 0.24987501011859603\n",
            "iteration 31400 / 75000: loss 0.20984689122432978\n",
            "iteration 31500 / 75000: loss 0.23653150870294146\n",
            "iteration 31600 / 75000: loss 0.19688809405366803\n",
            "iteration 31700 / 75000: loss 0.2594885314779954\n",
            "iteration 31800 / 75000: loss 0.19682225858925254\n",
            "iteration 31900 / 75000: loss 0.21399158817261374\n",
            "iteration 32000 / 75000: loss 0.2132504264030732\n",
            "iteration 32100 / 75000: loss 0.20489186541836737\n",
            "iteration 32200 / 75000: loss 0.2667875976968885\n",
            "iteration 32300 / 75000: loss 0.27418800025577417\n",
            "iteration 32400 / 75000: loss 0.21108653790926912\n",
            "iteration 32500 / 75000: loss 0.200973280740449\n",
            "iteration 32600 / 75000: loss 0.1694109547710737\n",
            "iteration 32700 / 75000: loss 0.18956676431955496\n",
            "iteration 32800 / 75000: loss 0.29409162261882477\n",
            "iteration 32900 / 75000: loss 0.24628169200306238\n",
            "iteration 33000 / 75000: loss 0.19948666624576514\n",
            "iteration 33100 / 75000: loss 0.2340786001448631\n",
            "iteration 33200 / 75000: loss 0.21307215605800583\n",
            "iteration 33300 / 75000: loss 0.19064419998180265\n",
            "iteration 33400 / 75000: loss 0.20855281105173495\n",
            "iteration 33500 / 75000: loss 0.19159389925391385\n",
            "iteration 33600 / 75000: loss 0.21525133973957453\n",
            "iteration 33700 / 75000: loss 0.19665254798225773\n",
            "iteration 33800 / 75000: loss 0.2056838129001857\n",
            "iteration 33900 / 75000: loss 0.2156776220921517\n",
            "iteration 34000 / 75000: loss 0.23637268385362165\n",
            "iteration 34100 / 75000: loss 0.25509577895280566\n",
            "iteration 34200 / 75000: loss 0.23393403487903447\n",
            "iteration 34300 / 75000: loss 0.17144375983400917\n",
            "iteration 34400 / 75000: loss 0.16391455562622914\n",
            "iteration 34500 / 75000: loss 0.17392659113450468\n",
            "iteration 34600 / 75000: loss 0.17553310117232068\n",
            "iteration 34700 / 75000: loss 0.20680485826078454\n",
            "iteration 34800 / 75000: loss 0.3069441727276998\n",
            "iteration 34900 / 75000: loss 0.22521716911937012\n",
            "iteration 35000 / 75000: loss 0.19204713920561012\n",
            "iteration 35100 / 75000: loss 0.1964565538197907\n",
            "iteration 35200 / 75000: loss 0.1647106406966659\n",
            "iteration 35300 / 75000: loss 0.22349747386310725\n",
            "iteration 35400 / 75000: loss 0.18855004432358155\n",
            "iteration 35500 / 75000: loss 0.22363599532445338\n",
            "iteration 35600 / 75000: loss 0.1614373927360506\n",
            "iteration 35700 / 75000: loss 0.18521233017649028\n",
            "iteration 35800 / 75000: loss 0.22350349818979307\n",
            "iteration 35900 / 75000: loss 0.22439372287953419\n",
            "iteration 36000 / 75000: loss 0.2131841941699125\n",
            "iteration 36100 / 75000: loss 0.23273273966272054\n",
            "iteration 36200 / 75000: loss 0.19182214397020536\n",
            "iteration 36300 / 75000: loss 0.20326083085719332\n",
            "iteration 36400 / 75000: loss 0.23174580183211246\n",
            "iteration 36500 / 75000: loss 0.21881978086454013\n",
            "iteration 36600 / 75000: loss 0.23994806789057468\n",
            "iteration 36700 / 75000: loss 0.16491311048772567\n",
            "iteration 36800 / 75000: loss 0.1538028217708242\n",
            "iteration 36900 / 75000: loss 0.21367933519881918\n",
            "iteration 37000 / 75000: loss 0.21519942012304494\n",
            "iteration 37100 / 75000: loss 0.19608757820145317\n",
            "iteration 37200 / 75000: loss 0.20148615701040595\n",
            "iteration 37300 / 75000: loss 0.19042176476535014\n",
            "iteration 37400 / 75000: loss 0.2643200385683311\n",
            "iteration 37500 / 75000: loss 0.14060016354580546\n",
            "iteration 37600 / 75000: loss 0.2345546769351743\n",
            "iteration 37700 / 75000: loss 0.2344667702141336\n",
            "iteration 37800 / 75000: loss 0.19880242338373516\n",
            "iteration 37900 / 75000: loss 0.27902816268138886\n",
            "iteration 38000 / 75000: loss 0.19899967023712342\n",
            "iteration 38100 / 75000: loss 0.26346922512198917\n",
            "iteration 38200 / 75000: loss 0.17479052906821657\n",
            "iteration 38300 / 75000: loss 0.1822130535571509\n",
            "iteration 38400 / 75000: loss 0.21218369092101566\n",
            "iteration 38500 / 75000: loss 0.20192999051690885\n",
            "iteration 38600 / 75000: loss 0.20085193932823303\n",
            "iteration 38700 / 75000: loss 0.2253874828038498\n",
            "iteration 38800 / 75000: loss 0.20663758104392044\n",
            "iteration 38900 / 75000: loss 0.24357729281981008\n",
            "iteration 39000 / 75000: loss 0.19517458307693758\n",
            "iteration 39100 / 75000: loss 0.1686775206730194\n",
            "iteration 39200 / 75000: loss 0.196617855784939\n",
            "iteration 39300 / 75000: loss 0.18356453407604134\n",
            "iteration 39400 / 75000: loss 0.23231008706233136\n",
            "iteration 39500 / 75000: loss 0.13711466547322057\n",
            "iteration 39600 / 75000: loss 0.20370615792120603\n",
            "iteration 39700 / 75000: loss 0.20094301992097585\n",
            "iteration 39800 / 75000: loss 0.23929810090005244\n",
            "iteration 39900 / 75000: loss 0.2001368633917703\n",
            "iteration 40000 / 75000: loss 0.18035235029019187\n",
            "iteration 40100 / 75000: loss 0.17270766455939085\n",
            "iteration 40200 / 75000: loss 0.2136488523498532\n",
            "iteration 40300 / 75000: loss 0.24100246283728805\n",
            "iteration 40400 / 75000: loss 0.17116517827971137\n",
            "iteration 40500 / 75000: loss 0.15999764105670894\n",
            "iteration 40600 / 75000: loss 0.14206264010431285\n",
            "iteration 40700 / 75000: loss 0.15688223469353396\n",
            "iteration 40800 / 75000: loss 0.19027537676039571\n",
            "iteration 40900 / 75000: loss 0.19566536787896494\n",
            "iteration 41000 / 75000: loss 0.21779555172060383\n",
            "iteration 41100 / 75000: loss 0.1574579430968405\n",
            "iteration 41200 / 75000: loss 0.2118650478777447\n",
            "iteration 41300 / 75000: loss 0.132606999924198\n",
            "iteration 41400 / 75000: loss 0.14711305526880847\n",
            "iteration 41500 / 75000: loss 0.2074018942874976\n",
            "iteration 41600 / 75000: loss 0.13825762546894868\n",
            "iteration 41700 / 75000: loss 0.2035758222921482\n",
            "iteration 41800 / 75000: loss 0.1766713283337712\n",
            "iteration 41900 / 75000: loss 0.19789063718407973\n",
            "iteration 42000 / 75000: loss 0.2051105450942753\n",
            "iteration 42100 / 75000: loss 0.19865805932975011\n",
            "iteration 42200 / 75000: loss 0.20575786200044993\n",
            "iteration 42300 / 75000: loss 0.1973464511007756\n",
            "iteration 42400 / 75000: loss 0.1847738464445281\n",
            "iteration 42500 / 75000: loss 0.18050620602988937\n",
            "iteration 42600 / 75000: loss 0.16863041236372306\n",
            "iteration 42700 / 75000: loss 0.18662603621852702\n",
            "iteration 42800 / 75000: loss 0.22451313471649748\n",
            "iteration 42900 / 75000: loss 0.16423731597833727\n",
            "iteration 43000 / 75000: loss 0.21196494868772814\n",
            "iteration 43100 / 75000: loss 0.152739917804614\n",
            "iteration 43200 / 75000: loss 0.21026166932797755\n",
            "iteration 43300 / 75000: loss 0.12912319497295316\n",
            "iteration 43400 / 75000: loss 0.18807126857154172\n",
            "iteration 43500 / 75000: loss 0.23036111491915073\n",
            "iteration 43600 / 75000: loss 0.17898890313347798\n",
            "iteration 43700 / 75000: loss 0.14775928373226774\n",
            "iteration 43800 / 75000: loss 0.202419785487218\n",
            "iteration 43900 / 75000: loss 0.1776749765319245\n",
            "iteration 44000 / 75000: loss 0.23135678995286205\n",
            "iteration 44100 / 75000: loss 0.20150031568584673\n",
            "iteration 44200 / 75000: loss 0.12569481545864997\n",
            "iteration 44300 / 75000: loss 0.17656660326705875\n",
            "iteration 44400 / 75000: loss 0.16789397774549655\n",
            "iteration 44500 / 75000: loss 0.22535171187586006\n",
            "iteration 44600 / 75000: loss 0.1939964697382722\n",
            "iteration 44700 / 75000: loss 0.1966362928059338\n",
            "iteration 44800 / 75000: loss 0.14718959661549316\n",
            "iteration 44900 / 75000: loss 0.1986918555891871\n",
            "iteration 45000 / 75000: loss 0.16564671466622855\n",
            "iteration 45100 / 75000: loss 0.22789092954166768\n",
            "iteration 45200 / 75000: loss 0.20253721835703645\n",
            "iteration 45300 / 75000: loss 0.22926737108082926\n",
            "iteration 45400 / 75000: loss 0.157827728925916\n",
            "iteration 45500 / 75000: loss 0.1441576453045235\n",
            "iteration 45600 / 75000: loss 0.19851454270417085\n",
            "iteration 45700 / 75000: loss 0.15832973594916447\n",
            "iteration 45800 / 75000: loss 0.17399880193324643\n",
            "iteration 45900 / 75000: loss 0.2084590440691846\n",
            "iteration 46000 / 75000: loss 0.17355844308105017\n",
            "iteration 46100 / 75000: loss 0.17933420039796455\n",
            "iteration 46200 / 75000: loss 0.17798980941956738\n",
            "iteration 46300 / 75000: loss 0.2153242471315408\n",
            "iteration 46400 / 75000: loss 0.20955366622898725\n",
            "iteration 46500 / 75000: loss 0.15847293477092478\n",
            "iteration 46600 / 75000: loss 0.15853688762467938\n",
            "iteration 46700 / 75000: loss 0.15734300282890828\n",
            "iteration 46800 / 75000: loss 0.193229263312666\n",
            "iteration 46900 / 75000: loss 0.19292226612302477\n",
            "iteration 47000 / 75000: loss 0.20019066439764507\n",
            "iteration 47100 / 75000: loss 0.20548659559883417\n",
            "iteration 47200 / 75000: loss 0.18994465317207018\n",
            "iteration 47300 / 75000: loss 0.19864705616648093\n",
            "iteration 47400 / 75000: loss 0.19967643181069045\n",
            "iteration 47500 / 75000: loss 0.16497011059536337\n",
            "iteration 47600 / 75000: loss 0.15594298480676735\n",
            "iteration 47700 / 75000: loss 0.15111262794945649\n",
            "iteration 47800 / 75000: loss 0.15603444210845974\n",
            "iteration 47900 / 75000: loss 0.17705873438264524\n",
            "iteration 48000 / 75000: loss 0.16699222510704778\n",
            "iteration 48100 / 75000: loss 0.1683572191655338\n",
            "iteration 48200 / 75000: loss 0.19746664616069687\n",
            "iteration 48300 / 75000: loss 0.2335534131463906\n",
            "iteration 48400 / 75000: loss 0.17649968694278362\n",
            "iteration 48500 / 75000: loss 0.1612933880306141\n",
            "iteration 48600 / 75000: loss 0.16019673151442237\n",
            "iteration 48700 / 75000: loss 0.22304916033480904\n",
            "iteration 48800 / 75000: loss 0.1343648727187357\n",
            "iteration 48900 / 75000: loss 0.21041717748582872\n",
            "iteration 49000 / 75000: loss 0.1818041130239224\n",
            "iteration 49100 / 75000: loss 0.21106993025373066\n",
            "iteration 49200 / 75000: loss 0.1494034145108532\n",
            "iteration 49300 / 75000: loss 0.16930959364011064\n",
            "iteration 49400 / 75000: loss 0.17148502067666166\n",
            "iteration 49500 / 75000: loss 0.2026990499877462\n",
            "iteration 49600 / 75000: loss 0.19876888263147885\n",
            "iteration 49700 / 75000: loss 0.1749235249656591\n",
            "iteration 49800 / 75000: loss 0.19358728898310207\n",
            "iteration 49900 / 75000: loss 0.1390375302840292\n",
            "iteration 50000 / 75000: loss 0.17621900574327592\n",
            "iteration 50100 / 75000: loss 0.1602233120301037\n",
            "iteration 50200 / 75000: loss 0.2057178065286485\n",
            "iteration 50300 / 75000: loss 0.15883082974181978\n",
            "iteration 50400 / 75000: loss 0.16356008064797\n",
            "iteration 50500 / 75000: loss 0.14834286812892922\n",
            "iteration 50600 / 75000: loss 0.1702644714486713\n",
            "iteration 50700 / 75000: loss 0.12350876248962417\n",
            "iteration 50800 / 75000: loss 0.14554717914473955\n",
            "iteration 50900 / 75000: loss 0.17870239020114978\n",
            "iteration 51000 / 75000: loss 0.15607713785568905\n",
            "iteration 51100 / 75000: loss 0.12858739144128445\n",
            "iteration 51200 / 75000: loss 0.19665787136513496\n",
            "iteration 51300 / 75000: loss 0.120401868491511\n",
            "iteration 51400 / 75000: loss 0.17598410445077442\n",
            "iteration 51500 / 75000: loss 0.16325485209821583\n",
            "iteration 51600 / 75000: loss 0.20741268898383958\n",
            "iteration 51700 / 75000: loss 0.16823171194785835\n",
            "iteration 51800 / 75000: loss 0.2197923819755083\n",
            "iteration 51900 / 75000: loss 0.1399806080115914\n",
            "iteration 52000 / 75000: loss 0.19747746218924117\n",
            "iteration 52100 / 75000: loss 0.17083764963299813\n",
            "iteration 52200 / 75000: loss 0.1650702369511008\n",
            "iteration 52300 / 75000: loss 0.1816583585921263\n",
            "iteration 52400 / 75000: loss 0.1555885317932482\n",
            "iteration 52500 / 75000: loss 0.18396163580985259\n",
            "iteration 52600 / 75000: loss 0.14477706416073366\n",
            "iteration 52700 / 75000: loss 0.22769966134839886\n",
            "iteration 52800 / 75000: loss 0.18561950431686375\n",
            "iteration 52900 / 75000: loss 0.18931372437268357\n",
            "iteration 53000 / 75000: loss 0.13622116562679432\n",
            "iteration 53100 / 75000: loss 0.19146922413391898\n",
            "iteration 53200 / 75000: loss 0.18550452391085898\n",
            "iteration 53300 / 75000: loss 0.2000589342545448\n",
            "iteration 53400 / 75000: loss 0.17320132772333457\n",
            "iteration 53500 / 75000: loss 0.17609675517823736\n",
            "iteration 53600 / 75000: loss 0.1738624978749637\n",
            "iteration 53700 / 75000: loss 0.16351909724021493\n",
            "iteration 53800 / 75000: loss 0.1558888029620394\n",
            "iteration 53900 / 75000: loss 0.16296318145420727\n",
            "iteration 54000 / 75000: loss 0.17272833376623475\n",
            "iteration 54100 / 75000: loss 0.17322766739506024\n",
            "iteration 54200 / 75000: loss 0.1822472777881931\n",
            "iteration 54300 / 75000: loss 0.1675914043646357\n",
            "iteration 54400 / 75000: loss 0.16053226676368235\n",
            "iteration 54500 / 75000: loss 0.18258750617318542\n",
            "iteration 54600 / 75000: loss 0.16427099834692294\n",
            "iteration 54700 / 75000: loss 0.18631699958201486\n",
            "iteration 54800 / 75000: loss 0.1608731372227211\n",
            "iteration 54900 / 75000: loss 0.22979133920263864\n",
            "iteration 55000 / 75000: loss 0.18677886605852884\n",
            "iteration 55100 / 75000: loss 0.18198433221556137\n",
            "iteration 55200 / 75000: loss 0.15305602353295242\n",
            "iteration 55300 / 75000: loss 0.16934313814175966\n",
            "iteration 55400 / 75000: loss 0.1853357537127422\n",
            "iteration 55500 / 75000: loss 0.19200111698643163\n",
            "iteration 55600 / 75000: loss 0.15970256345686756\n",
            "iteration 55700 / 75000: loss 0.18484059522609256\n",
            "iteration 55800 / 75000: loss 0.1564643346424598\n",
            "iteration 55900 / 75000: loss 0.15737615316145886\n",
            "iteration 56000 / 75000: loss 0.16326670710229785\n",
            "iteration 56100 / 75000: loss 0.18281646967796022\n",
            "iteration 56200 / 75000: loss 0.13668655858516943\n",
            "iteration 56300 / 75000: loss 0.1756911543224125\n",
            "iteration 56400 / 75000: loss 0.16742038344003207\n",
            "iteration 56500 / 75000: loss 0.15025750133585325\n",
            "iteration 56600 / 75000: loss 0.17846062623866307\n",
            "iteration 56700 / 75000: loss 0.17605771263788964\n",
            "iteration 56800 / 75000: loss 0.15886263790822114\n",
            "iteration 56900 / 75000: loss 0.13189352949951583\n",
            "iteration 57000 / 75000: loss 0.13897026545016322\n",
            "iteration 57100 / 75000: loss 0.17846306087128577\n",
            "iteration 57200 / 75000: loss 0.18108977908363563\n",
            "iteration 57300 / 75000: loss 0.14879369934471262\n",
            "iteration 57400 / 75000: loss 0.13671880377601484\n",
            "iteration 57500 / 75000: loss 0.11968189700170531\n",
            "iteration 57600 / 75000: loss 0.1731873037935962\n",
            "iteration 57700 / 75000: loss 0.16924227369038275\n",
            "iteration 57800 / 75000: loss 0.1608362712575796\n",
            "iteration 57900 / 75000: loss 0.15948280460431125\n",
            "iteration 58000 / 75000: loss 0.21119923253293185\n",
            "iteration 58100 / 75000: loss 0.18895553379992713\n",
            "iteration 58200 / 75000: loss 0.18915177707163605\n",
            "iteration 58300 / 75000: loss 0.16725886341417529\n",
            "iteration 58400 / 75000: loss 0.17080102193137858\n",
            "iteration 58500 / 75000: loss 0.17208266334577074\n",
            "iteration 58600 / 75000: loss 0.15861913731147054\n",
            "iteration 58700 / 75000: loss 0.22192552779159364\n",
            "iteration 58800 / 75000: loss 0.1576191541877362\n",
            "iteration 58900 / 75000: loss 0.1673410540072055\n",
            "iteration 59000 / 75000: loss 0.19692173759457332\n",
            "iteration 59100 / 75000: loss 0.1869992644287319\n",
            "iteration 59200 / 75000: loss 0.14432606708219353\n",
            "iteration 59300 / 75000: loss 0.1499273012242855\n",
            "iteration 59400 / 75000: loss 0.20155329629712776\n",
            "iteration 59500 / 75000: loss 0.15626886402028367\n",
            "iteration 59600 / 75000: loss 0.15472213465328072\n",
            "iteration 59700 / 75000: loss 0.15616925174353613\n",
            "iteration 59800 / 75000: loss 0.13635473146710778\n",
            "iteration 59900 / 75000: loss 0.14656575350457038\n",
            "iteration 60000 / 75000: loss 0.15794048636198998\n",
            "iteration 60100 / 75000: loss 0.12994285469824676\n",
            "iteration 60200 / 75000: loss 0.1319439427048889\n",
            "iteration 60300 / 75000: loss 0.17087455337783075\n",
            "iteration 60400 / 75000: loss 0.12581207264965807\n",
            "iteration 60500 / 75000: loss 0.1299201235505334\n",
            "iteration 60600 / 75000: loss 0.19042962491430948\n",
            "iteration 60700 / 75000: loss 0.15040010630872244\n",
            "iteration 60800 / 75000: loss 0.1317090502058571\n",
            "iteration 60900 / 75000: loss 0.11888602558591754\n",
            "iteration 61000 / 75000: loss 0.14252492860332347\n",
            "iteration 61100 / 75000: loss 0.15155008984263085\n",
            "iteration 61200 / 75000: loss 0.1292143006750918\n",
            "iteration 61300 / 75000: loss 0.11264469214117218\n",
            "iteration 61400 / 75000: loss 0.14145876528127027\n",
            "iteration 61500 / 75000: loss 0.1295223579993927\n",
            "iteration 61600 / 75000: loss 0.2070675121071759\n",
            "iteration 61700 / 75000: loss 0.20722596903913126\n",
            "iteration 61800 / 75000: loss 0.1872066149824131\n",
            "iteration 61900 / 75000: loss 0.13403986552449346\n",
            "iteration 62000 / 75000: loss 0.12425376797243079\n",
            "iteration 62100 / 75000: loss 0.16663085153510535\n",
            "iteration 62200 / 75000: loss 0.10761266809178507\n",
            "iteration 62300 / 75000: loss 0.1744449224250294\n",
            "iteration 62400 / 75000: loss 0.1680830185640791\n",
            "iteration 62500 / 75000: loss 0.14493808391480173\n",
            "iteration 62600 / 75000: loss 0.16015740877421222\n",
            "iteration 62700 / 75000: loss 0.14619413351135624\n",
            "iteration 62800 / 75000: loss 0.17393227302095637\n",
            "iteration 62900 / 75000: loss 0.18093423275294704\n",
            "iteration 63000 / 75000: loss 0.13904980876426137\n",
            "iteration 63100 / 75000: loss 0.16535932470735787\n",
            "iteration 63200 / 75000: loss 0.11954839501642996\n",
            "iteration 63300 / 75000: loss 0.1447769896013668\n",
            "iteration 63400 / 75000: loss 0.1361286105552337\n",
            "iteration 63500 / 75000: loss 0.13246931296069367\n",
            "iteration 63600 / 75000: loss 0.172266359784366\n",
            "iteration 63700 / 75000: loss 0.11395767870279355\n",
            "iteration 63800 / 75000: loss 0.13447291538260525\n",
            "iteration 63900 / 75000: loss 0.14003547131489735\n",
            "iteration 64000 / 75000: loss 0.16899509045838387\n",
            "iteration 64100 / 75000: loss 0.17357812814059792\n",
            "iteration 64200 / 75000: loss 0.13288360892706724\n",
            "iteration 64300 / 75000: loss 0.16111799382042152\n",
            "iteration 64400 / 75000: loss 0.11763709647490522\n",
            "iteration 64500 / 75000: loss 0.16397445429233115\n",
            "iteration 64600 / 75000: loss 0.10376733054154531\n",
            "iteration 64700 / 75000: loss 0.13289674119189956\n",
            "iteration 64800 / 75000: loss 0.14303466406498147\n",
            "iteration 64900 / 75000: loss 0.17889603900670692\n",
            "iteration 65000 / 75000: loss 0.09914116325618516\n",
            "iteration 65100 / 75000: loss 0.13465765196287938\n",
            "iteration 65200 / 75000: loss 0.14490207381814085\n",
            "iteration 65300 / 75000: loss 0.20692934144708502\n",
            "iteration 65400 / 75000: loss 0.1680861810259698\n",
            "iteration 65500 / 75000: loss 0.1573271375024635\n",
            "iteration 65600 / 75000: loss 0.19634337100539656\n",
            "iteration 65700 / 75000: loss 0.14779957470603233\n",
            "iteration 65800 / 75000: loss 0.15703128006851566\n",
            "iteration 65900 / 75000: loss 0.14267350130379414\n",
            "iteration 66000 / 75000: loss 0.16064071877620892\n",
            "iteration 66100 / 75000: loss 0.17382323012591938\n",
            "iteration 66200 / 75000: loss 0.15015041154795636\n",
            "iteration 66300 / 75000: loss 0.1896495232322949\n",
            "iteration 66400 / 75000: loss 0.1586340864511182\n",
            "iteration 66500 / 75000: loss 0.1401905709714112\n",
            "iteration 66600 / 75000: loss 0.12767258044148805\n",
            "iteration 66700 / 75000: loss 0.14451972039832398\n",
            "iteration 66800 / 75000: loss 0.18176874756092246\n",
            "iteration 66900 / 75000: loss 0.2230621533542826\n",
            "iteration 67000 / 75000: loss 0.10375131606133137\n",
            "iteration 67100 / 75000: loss 0.1740561230871242\n",
            "iteration 67200 / 75000: loss 0.11484851317990787\n",
            "iteration 67300 / 75000: loss 0.1523732180426214\n",
            "iteration 67400 / 75000: loss 0.1410633885627779\n",
            "iteration 67500 / 75000: loss 0.14364001915012664\n",
            "iteration 67600 / 75000: loss 0.1653376076888599\n",
            "iteration 67700 / 75000: loss 0.20552378759462459\n",
            "iteration 67800 / 75000: loss 0.16896550983332492\n",
            "iteration 67900 / 75000: loss 0.13790462445381074\n",
            "iteration 68000 / 75000: loss 0.14290085010533565\n",
            "iteration 68100 / 75000: loss 0.1377139166395377\n",
            "iteration 68200 / 75000: loss 0.14678081821040734\n",
            "iteration 68300 / 75000: loss 0.19317247500769247\n",
            "iteration 68400 / 75000: loss 0.14859287467122972\n",
            "iteration 68500 / 75000: loss 0.17494791700274703\n",
            "iteration 68600 / 75000: loss 0.15309114523119563\n",
            "iteration 68700 / 75000: loss 0.1289661398875649\n",
            "iteration 68800 / 75000: loss 0.15509233586892457\n",
            "iteration 68900 / 75000: loss 0.18324080691407765\n",
            "iteration 69000 / 75000: loss 0.09353252911159016\n",
            "iteration 69100 / 75000: loss 0.11132394393554255\n",
            "iteration 69200 / 75000: loss 0.13248008774237718\n",
            "iteration 69300 / 75000: loss 0.12016255305057709\n",
            "iteration 69400 / 75000: loss 0.16177702968306557\n",
            "iteration 69500 / 75000: loss 0.12446900280564327\n",
            "iteration 69600 / 75000: loss 0.2350829001951855\n",
            "iteration 69700 / 75000: loss 0.15501047784225064\n",
            "iteration 69800 / 75000: loss 0.12695777898427774\n",
            "iteration 69900 / 75000: loss 0.1230785164700034\n",
            "iteration 70000 / 75000: loss 0.12502443969983218\n",
            "iteration 70100 / 75000: loss 0.1307398645937657\n",
            "iteration 70200 / 75000: loss 0.12933838796169164\n",
            "iteration 70300 / 75000: loss 0.1575452362796907\n",
            "iteration 70400 / 75000: loss 0.19007546465659203\n",
            "iteration 70500 / 75000: loss 0.12839560830774896\n",
            "iteration 70600 / 75000: loss 0.16206444934764214\n",
            "iteration 70700 / 75000: loss 0.15058249797848214\n",
            "iteration 70800 / 75000: loss 0.11779070367637337\n",
            "iteration 70900 / 75000: loss 0.1490666983370154\n",
            "iteration 71000 / 75000: loss 0.1333642236729996\n",
            "iteration 71100 / 75000: loss 0.14503720386348198\n",
            "iteration 71200 / 75000: loss 0.2144643989078642\n",
            "iteration 71300 / 75000: loss 0.15632937698682553\n",
            "iteration 71400 / 75000: loss 0.14721965142725785\n",
            "iteration 71500 / 75000: loss 0.12985851384948768\n",
            "iteration 71600 / 75000: loss 0.1245095645713154\n",
            "iteration 71700 / 75000: loss 0.14643162573469534\n",
            "iteration 71800 / 75000: loss 0.2131124130327533\n",
            "iteration 71900 / 75000: loss 0.16021793180662092\n",
            "iteration 72000 / 75000: loss 0.11618092901805757\n",
            "iteration 72100 / 75000: loss 0.1434379431378573\n",
            "iteration 72200 / 75000: loss 0.15298807111631593\n",
            "iteration 72300 / 75000: loss 0.12228609226900221\n",
            "iteration 72400 / 75000: loss 0.11978384053915246\n",
            "iteration 72500 / 75000: loss 0.14405981648493163\n",
            "iteration 72600 / 75000: loss 0.14954029774107283\n",
            "iteration 72700 / 75000: loss 0.14202498355239215\n",
            "iteration 72800 / 75000: loss 0.13439639905862774\n",
            "iteration 72900 / 75000: loss 0.16342245996116395\n",
            "iteration 73000 / 75000: loss 0.12613135674921228\n",
            "iteration 73100 / 75000: loss 0.13117569440411334\n",
            "iteration 73200 / 75000: loss 0.15781501212636917\n",
            "iteration 73300 / 75000: loss 0.11591548215368644\n",
            "iteration 73400 / 75000: loss 0.125978562709459\n",
            "iteration 73500 / 75000: loss 0.14093777611401176\n",
            "iteration 73600 / 75000: loss 0.10971721824724766\n",
            "iteration 73700 / 75000: loss 0.18841587791006215\n",
            "iteration 73800 / 75000: loss 0.1713082818308273\n",
            "iteration 73900 / 75000: loss 0.13150871722225432\n",
            "iteration 74000 / 75000: loss 0.1359683553439084\n",
            "iteration 74100 / 75000: loss 0.14023491813979108\n",
            "iteration 74200 / 75000: loss 0.13875024943255992\n",
            "iteration 74300 / 75000: loss 0.14857449194905548\n",
            "iteration 74400 / 75000: loss 0.09191287825436309\n",
            "iteration 74500 / 75000: loss 0.16374606369363764\n",
            "iteration 74600 / 75000: loss 0.17190476058528834\n",
            "iteration 74700 / 75000: loss 0.17464113708817608\n",
            "iteration 74800 / 75000: loss 0.11600439809768064\n",
            "iteration 74900 / 75000: loss 0.1740283515172976\n",
            "Accuracy scikit-learn: 0.9733333333333334\n",
            "Accuracy gradient descent model : 0.96\n"
          ]
        }
      ],
      "source": [
        "sk_model = LogisticRegression(fit_intercept=True)\n",
        "sk_model.fit(X_train2, y_train2)\n",
        "sk_pred = sk_model.predict(X_train2)\n",
        "sk_accuracy = accuracy_score(y_train2, sk_pred)\n",
        "\n",
        "model = MultinomialLogisticRegressor(fit_intercept=True)\n",
        "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
        "pred = model.predict(X_train2)\n",
        "model_accuracy = accuracy_score(y_train2, pred)\n",
        "\n",
        "print(\"Accuracy scikit-learn:\", sk_accuracy)\n",
        "print(\"Accuracy gradient descent model :\", model_accuracy)\n",
        "assert sk_accuracy - model_accuracy < 0.02"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}