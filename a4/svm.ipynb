{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9970b93e",
   "metadata": {},
   "source": [
    "## MISA (2024-2025)\n",
    "- Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia redémarrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
    "\n",
    "- Izay misy hoe `YOUR CODE HERE` na `YOUR ANSWER HERE` ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f5420",
   "metadata": {},
   "source": [
    "## References\n",
    "* [PCA avec SVD](https://www.google.com/search?q=pca+avec+svd&oq=pca+avec+svd&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRifBTIHCAQQIRifBTIHCAUQIRifBTIHCAYQIRifBTIHCAcQIRifBTIHCAgQIRifBTIHCAkQIRifBdIBCDUwMjBqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8#fpstate=ive&vld=cid:c7c3b00d,vid:ILu4-Lk-gZQ,st:0)\n",
    "* [cvxpy](https://www.cvxpy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b6d9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4eec0cbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7791ca10232655ac53dd4bba44138d2",
     "grade": false,
     "grade_id": "cell-bf951004994ebba2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition  import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing, load_iris, load_breast_cancer, make_blobs\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cvxpy as cp\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08411e-b444-4c68-8063-fc96f5ddda8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ef2dd",
   "metadata": {},
   "source": [
    "#### pour la recherche des vecteurs propres on peut utiliser le SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e8e97248",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9120e24f30de93edd4cc9644ab9d801f",
     "grade": false,
     "grade_id": "cell-6aa2faaa76e6b08e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrincipalComponentAnalysis:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        # Centrer les données\n",
    "        X_centered = X - np.mean(X, axis=0)\n",
    "        \n",
    "        # Calculer la SVD\n",
    "        U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "        \n",
    "        # Les composantes principales sont dans Vt\n",
    "        self.components = Vt[:self.n_components]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Projeter les données sur les composantes principales\n",
    "        X_centered = X - np.mean(X, axis=0)\n",
    "        return np.dot(X_centered, self.components.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c46e59cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9b88bb52accd5de97db2f2e3cd97c67",
     "grade": true,
     "grade_id": "cell-5e021e5196801706",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.99364782835624e-14\n"
     ]
    }
   ],
   "source": [
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_centered)\n",
    "X_pca_trans = pca.transform(X_centered)\n",
    "\n",
    "model = PrincipalComponentAnalysis(n_components=3)\n",
    "model.fit(X_centered)\n",
    "X_model_trans = model.transform(X_centered)\n",
    "\n",
    "sign_correct_X_model_trans = np.concatenate([np.expand_dims(X_model_trans[:,0],axis=1),-X_model_trans[:,1:]],axis=1)\n",
    "\n",
    "error = rel_error(X_pca_trans, sign_correct_X_model_trans)\n",
    "print(error)\n",
    "assert  error < 1e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae54f61",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Binary Linear SVM with CVXPY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f038eeec",
   "metadata": {},
   "source": [
    "## Hard margin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ae0eae3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67a78e61592d16446bae3f77e17ceaea",
     "grade": false,
     "grade_id": "cell-225fb5eb2216ae3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X2, y2 = make_blobs(n_samples=300, centers=2, n_features=12, random_state=47)\n",
    "scaler = StandardScaler()\n",
    "X2 = scaler.fit_transform(X2)\n",
    "y2[y2 == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1bc8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb10a2e1c603f3c29915c7cac6bf7866",
     "grade": false,
     "grade_id": "cell-b459cc87f3d8726e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$$\\min_{\\mathbf{w},b}\\frac{1}{2}||\\mathbf{w}||^2$$ <br>\n",
    "$$\\text{s.t } y_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b) \\ge 1, \\ i=1...N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "829be3d2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bcc3a988e383b297c2805d1f6aca7dc",
     "grade": false,
     "grade_id": "cell-d04165c70ffbb870",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "class LinearSVMHardMargin:\n",
    "    def __init__(self):\n",
    "        self.w = None  # Le vecteur normal à l'hyperplan\n",
    "        self.b = 0  # Le biais de l'hyperplan\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # X : les données d'entrée (n_samples, n_features)\n",
    "        # y : les labels (n_samples,)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Définir les variables du problème d'optimisation\n",
    "        w = cp.Variable(n_features)\n",
    "        b = cp.Variable()\n",
    "        \n",
    "        # Définir les contraintes : y_i * (w^T x_i + b) >= 1\n",
    "        constraints = [y[i] * (X[i] @ w + b) >= 1 for i in range(n_samples)]\n",
    "        \n",
    "        # Définir l'objectif : minimiser (1/2) * ||w||^2\n",
    "        objective = cp.Minimize(0.5 * cp.sum_squares(w))  # Utiliser sum_squares pour calculer la norme\n",
    "\n",
    "        # Définir et résoudre le problème d'optimisation\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        problem.solve()\n",
    "        \n",
    "        # Stocker les résultats dans les attributs de la classe\n",
    "        self.w = w.value\n",
    "        self.b = b.value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Retourne les étiquettes prédites (1 ou -1)\"\"\"\n",
    "        # Prédire en fonction du signe de (X @ w + b)\n",
    "        return np.sign(X @ self.w + self.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "51c5cf5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fcf9bdd4f28f6b42218539a988b638b",
     "grade": true,
     "grade_id": "cell-5d1f0df83bcb86d1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVMHardMargin()\n",
    "model.fit(X2, y2)\n",
    "pred = model.predict(X2)\n",
    "accuracy = accuracy_score(y2, pred)\n",
    "print(accuracy)\n",
    "assert accuracy == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3371a",
   "metadata": {},
   "source": [
    "## Soft Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "03e3bfe9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14637e9c774165ae0c123891964870b3",
     "grade": false,
     "grade_id": "cell-e1ffa5b41b5d9d99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data3 = load_breast_cancer()\n",
    "X3, y3 = data3.data, data3.target\n",
    "scaler = StandardScaler()\n",
    "X3 = scaler.fit_transform(X3)\n",
    "y3[y3 == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86939d9a",
   "metadata": {},
   "source": [
    "$$L(\\mathbf{w},b) = \\frac{1}{N} \\sum_{i=1}^N \\max(0, y_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)) + \\lambda||\\mathbf{w}||^2_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "118e55f7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6723ed1e25d3de291f7156530cf97613",
     "grade": false,
     "grade_id": "cell-dff3e00a210cd783",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "class LinearSVMSoftMargin:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.w = None  # Vecteur normal à l'hyperplan\n",
    "        self.b = 0     # Biais de l'hyperplan\n",
    "        self.alpha = alpha  # Paramètre C, qui est le poids de la pénalité pour les erreurs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Définir les variables de décision\n",
    "        w = cp.Variable(n_features)\n",
    "        b = cp.Variable()\n",
    "        \n",
    "        # Calculer la perte Hinge\n",
    "        hinge_loss = cp.maximum(0, 1 - cp.multiply(y, X @ w + b))\n",
    "        \n",
    "        # Définir l'objectif : minimiser la moyenne de la perte Hinge + régularisation\n",
    "        objective = cp.Minimize(\n",
    "            cp.mean(hinge_loss) + self.alpha * cp.sum_squares(w)\n",
    "        )\n",
    "        \n",
    "        # Définir et résoudre le problème d'optimisation\n",
    "        problem = cp.Problem(objective)\n",
    "        problem.solve()\n",
    "        \n",
    "        # Stocker les résultats dans les attributs de la classe\n",
    "        self.w = w.value\n",
    "        self.b = b.value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Retourne les étiquettes prédites (1 ou -1)\"\"\"\n",
    "        # Prédire en fonction du signe de (X @ w + b)\n",
    "        return np.sign(X @ self.w + self.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "716b990e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c58313a6f325f89e25254d2bc3aecae",
     "grade": true,
     "grade_id": "cell-915364d1b5f54b7e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVMSoftMargin(alpha=1e-3)\n",
    "model.fit(X3, y3)\n",
    "pred = model.predict(X3)\n",
    "accuracy = accuracy_score(y3, pred)\n",
    "print(accuracy)\n",
    "assert accuracy >= 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3b15c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multiclass Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549dff61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de9d832",
   "metadata": {},
   "source": [
    "$$L(\\mathbf{W}) = \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1) + \\lambda||\\mathbf{w}||^2_2$$ <br>\n",
    "$$\\text{where } s_j = (f(\\mathbf{x}_i;\\mathbf{W}))_j = (\\mathbf{W}\\mathbf{x}_i)_j \\text{ is the score for the j-th class}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6d4d5e7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12b5f742a05bdaf1b6adf2bb6083dcd3",
     "grade": false,
     "grade_id": "cell-67c81d664d204471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "W = np.random.randn(X.shape[1], 3) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fd15eccd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53138b3da2e0d48438be8ea853f14eca",
     "grade": false,
     "grade_id": "cell-a015ff237ad977a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def svm_loss_naive(W, X, y, alpha):\n",
    "    \"\"\"\n",
    "    Fonction de perte multiclasses SVM avec boucles for\n",
    "\n",
    "    Inputs:\n",
    "    - W: tableau de forme (D, C) contenant les poids\n",
    "    - X: tableau de forme (N, D) contenant un lot de données\n",
    "    - y: tableau de forme (N,) contenant les étiquettes des données\n",
    "    - alpha: (float) régularisation des poids\n",
    "\n",
    "    Returns:\n",
    "    - loss: une seule valeur, la perte totale\n",
    "    - dW: gradient par rapport aux poids W; même forme que W\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialisation de la perte et du gradient\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    N = X.shape[0]  # Nombre d'exemples\n",
    "    C = W.shape[1]  # Nombre de classes\n",
    "    \n",
    "    # Parcourir tous les exemples\n",
    "    for i in range(N):\n",
    "        scores = X[i].dot(W)  # Calcul des scores pour chaque classe\n",
    "        correct_class_score = scores[y[i]]  # Score de la classe correcte\n",
    "        \n",
    "        # Parcourir toutes les classes\n",
    "        for j in range(C):\n",
    "            if j == y[i]:\n",
    "                continue  # On saute la classe correcte\n",
    "\n",
    "            margin = scores[j] - correct_class_score + 1  # Calcul de la perte de marge Hinge\n",
    "\n",
    "            # Si la marge est positive, ajouter à la perte\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                # Calcul du gradient\n",
    "                dW[:, j] += X[i]  # Poids de la classe j\n",
    "                dW[:, y[i]] -= X[i]  # Poids de la classe correcte\n",
    "\n",
    "    # Moyenne de la perte sur tous les exemples\n",
    "    loss /= N\n",
    "    \n",
    "    # Ajouter la régularisation (penalisation des grands poids)\n",
    "    loss += 0.5 * alpha * np.sum(W * W)  # np.sum(W * W) calcule ||W||^2\n",
    "\n",
    "    # Moyenne du gradient sur tous les exemples\n",
    "    dW /= N\n",
    "    \n",
    "    # Ajouter la régularisation au gradient\n",
    "    dW += alpha * W  # Gradient de la régularisation\n",
    "\n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "39b07bb6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34ca0a0a01822fa2a097921b9df2b6c5",
     "grade": true,
     "grade_id": "cell-94feff1af9a3c069",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.296000 analytic: 2.296000, relative error: 2.772656e-12\n",
      "numerical: 0.953333 analytic: 0.953333, relative error: 6.972861e-12\n",
      "numerical: -0.502000 analytic: -0.502000, relative error: 8.843524e-12\n",
      "numerical: -0.370667 analytic: -0.370667, relative error: 6.389224e-12\n",
      "numerical: -0.126667 analytic: -0.126667, relative error: 3.503670e-10\n",
      "numerical: 2.296000 analytic: 2.296000, relative error: 2.772656e-12\n",
      "numerical: -1.794000 analytic: -1.794000, relative error: 1.653576e-13\n",
      "numerical: 0.083333 analytic: 0.083333, relative error: 6.334458e-11\n",
      "numerical: -1.794000 analytic: -1.794000, relative error: 1.653576e-13\n",
      "numerical: -0.502000 analytic: -0.502000, relative error: 8.843524e-12\n",
      "numerical: -0.092667 analytic: -0.092667, relative error: 8.346823e-11\n",
      "numerical: -1.794000 analytic: -1.794000, relative error: 1.653576e-13\n"
     ]
    }
   ],
   "source": [
    "# NO REGLARIZATION\n",
    "loss, dW = svm_loss_naive(W, X, y, 0.0)\n",
    "\n",
    "f = lambda W: svm_loss_naive(W, X, y, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4dd2699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.287405 analytic: 0.287405, relative error: 7.303629e-12\n",
      "numerical: -1.793824 analytic: -1.793824, relative error: 1.125062e-12\n",
      "numerical: -0.826779 analytic: -0.826779, relative error: 3.517214e-11\n",
      "numerical: -0.744808 analytic: -0.744808, relative error: 6.420085e-12\n",
      "numerical: -0.126603 analytic: -0.126603, relative error: 4.089537e-10\n",
      "numerical: -1.793824 analytic: -1.793824, relative error: 1.125062e-12\n",
      "numerical: 0.287405 analytic: 0.287405, relative error: 7.303629e-12\n",
      "numerical: 0.953445 analytic: 0.953445, relative error: 4.232425e-12\n",
      "numerical: 0.083117 analytic: 0.083117, relative error: 4.548635e-11\n",
      "numerical: -0.126603 analytic: -0.126603, relative error: 4.089537e-10\n",
      "numerical: -0.370497 analytic: -0.370497, relative error: 8.241184e-12\n",
      "numerical: 0.953445 analytic: 0.953445, relative error: 4.232425e-12\n"
     ]
    }
   ],
   "source": [
    "#REGLARIZATION\n",
    "loss, dW = svm_loss_naive(W, X, y, 2)\n",
    "\n",
    "f = lambda W: svm_loss_naive(W, X, y, 2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3ecca1d8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5ebb303976d8369a9bb802d3976cad1",
     "grade": false,
     "grade_id": "cell-7e17d2c0c1de2480",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svm_loss_vectorized(W, X, y, alpha):\n",
    "    \"\"\"\n",
    "    Multiclass SVM loss function WITHOUT FOR LOOPS\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (D, C) containing weights\n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels\n",
    "    - alpha: (float) regularization \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    C = W.shape[1]\n",
    "    \n",
    "    # Compute the scores (N, C)\n",
    "    S = X @ W\n",
    "    \n",
    "    # Indices for the correct classes\n",
    "    idx = np.arange(N)\n",
    "    \n",
    "    # Calculate the margins (N, C)\n",
    "    margins = np.maximum(0, S.T - S[idx, y] + 1).T\n",
    "    margins[idx, y] = 0  # Set the margin for the correct class to 0\n",
    "    \n",
    "    # Create a matrix where margin > 0 is 1, else 0 (Violation of margin)\n",
    "    margin_violations = np.where(margins > 0, 1, 0)\n",
    "    \n",
    "    # Create a matrix to store the sum of violations for the correct class\n",
    "    correct_class_violations = np.zeros_like(margin_violations)\n",
    "    correct_class_violations[idx, y] = np.sum(margin_violations, axis=1)\n",
    "    \n",
    "    # Compute the gradient (D, C)\n",
    "    dW += X.T @ margin_violations\n",
    "    dW -= X.T @ correct_class_violations\n",
    "    \n",
    "    # Calculate the loss (average margin + regularization)\n",
    "    loss = np.sum(margins) # Average margin across all examples\n",
    "    loss += 0.5 * alpha * np.sum(W**2)  # Add regularization term\n",
    "    \n",
    "    # Add regularization gradient\n",
    "    dW += alpha * W\n",
    "    \n",
    "    return loss, dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0fa38170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "884d69e79b74e6756f42d7927309de82",
     "grade": true,
     "grade_id": "cell-1bf11ff8259d6847",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 125.600000 analytic: 125.600000, relative error: 4.831860e-12\n",
      "numerical: -269.100000 analytic: -269.100000, relative error: 5.033315e-12\n",
      "numerical: 143.000000 analytic: 143.000000, relative error: 9.641717e-12\n",
      "numerical: -55.600000 analytic: -55.600000, relative error: 3.554202e-11\n",
      "numerical: -111.700000 analytic: -111.700000, relative error: 2.865135e-12\n",
      "numerical: -75.300000 analytic: -75.300000, relative error: 1.680467e-11\n",
      "numerical: -111.700000 analytic: -111.700000, relative error: 2.865135e-12\n",
      "numerical: -269.100000 analytic: -269.100000, relative error: 5.033315e-12\n",
      "numerical: 12.500000 analytic: 12.500000, relative error: 1.255126e-10\n",
      "numerical: -19.000000 analytic: -19.000000, relative error: 6.567547e-11\n",
      "numerical: -55.600000 analytic: -55.600000, relative error: 3.554202e-11\n",
      "numerical: -269.100000 analytic: -269.100000, relative error: 5.033315e-12\n"
     ]
    }
   ],
   "source": [
    "# NO REGLARIZATION\n",
    "loss, dW = svm_loss_vectorized(W, X, y, 0.0)\n",
    "\n",
    "f = lambda W: svm_loss_vectorized(W, X, y, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "66b82330",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53c5c62906c11829422603614eae7ba7",
     "grade": true,
     "grade_id": "cell-33078afde070ae4f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -124.000112 analytic: -124.000112, relative error: 6.664713e-12\n",
      "numerical: -111.700141 analytic: -111.700141, relative error: 1.813249e-12\n",
      "numerical: -269.099824 analytic: -269.099824, relative error: 7.956183e-12\n",
      "numerical: -18.999936 analytic: -18.999936, relative error: 4.425060e-11\n",
      "numerical: 43.100072 analytic: 43.100072, relative error: 4.169692e-12\n",
      "numerical: 43.100072 analytic: 43.100072, relative error: 4.169692e-12\n",
      "numerical: 12.499784 analytic: 12.499784, relative error: 3.479915e-11\n",
      "numerical: -75.300471 analytic: -75.300471, relative error: 2.798417e-11\n",
      "numerical: -13.900196 analytic: -13.900196, relative error: 8.172583e-11\n",
      "numerical: 12.499784 analytic: 12.499784, relative error: 3.479915e-11\n",
      "numerical: 143.000111 analytic: 143.000111, relative error: 1.594864e-11\n",
      "numerical: -13.900196 analytic: -13.900196, relative error: 8.172583e-11\n"
     ]
    }
   ],
   "source": [
    "# REGLARIZATION\n",
    "loss, dW = svm_loss_vectorized(W, X, y, 2)\n",
    "\n",
    "f = lambda W: svm_loss_vectorized(W, X, y, 2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55615d17",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366dcd62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f1639f76b795c999d467767430e2778",
     "grade": false,
     "grade_id": "cell-41975286849a6d83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearModel():\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.W = None\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))  # Ajouter une colonne de 1 pour l'interception\n",
    "\n",
    "        N, d = X.shape\n",
    "        C = np.max(y) + 1  # Nombre de classes\n",
    "        if self.W is None:  # Initialisation des poids\n",
    "            self.W = 0.001 * np.random.randn(d, C)  # Initialisation aléatoire des poids\n",
    "\n",
    "        # Exécution de la descente de gradient stochastique pour optimiser W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            # Sélectionner un mini-batch\n",
    "            indices = np.random.choice(N, batch_size, replace=False)  # Échantillonner batch_size éléments\n",
    "            X_batch = X[indices, :]  # Récupérer les données du mini-batch\n",
    "            y_batch = y[indices]  # Récupérer les labels du mini-batch\n",
    "\n",
    "            # Évaluer la perte et le gradient\n",
    "            loss, dW = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Mise à jour des poids\n",
    "            self.W -= learning_rate * dW  # Mettre à jour les poids avec la descente de gradient\n",
    "\n",
    "            if verbose and it % 1000 == 0:\n",
    "                print(f\"iteration {it} / {num_iters}: loss {loss}\")\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearSVM(LinearModel):\n",
    "    \"\"\" Modèle SVM linéaire \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return svm_loss_vectorized(self.W, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Effectue la prédiction avec les poids actuels\n",
    "\n",
    "        Inputs:\n",
    "        - X: array de forme (N, D) contenant les données d'entrée\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: array unidimensionnel de taille N, chaque élément représentant la classe prédite\n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0], 1)), X))  # Ajouter une colonne de 1 pour l'interception\n",
    "\n",
    "        scores = X @ self.W  # Calculer les scores en fonction des poids\n",
    "        y_pred = np.argmax(scores, axis=1)  # Prendre la classe ayant le score le plus élevé pour chaque exemple\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "344add9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aee08beb636f046b69da3ada47961ae6",
     "grade": true,
     "grade_id": "cell-f1cb5669bcaed283",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 127.65063841826927\n",
      "iteration 1000 / 75000: loss 2.4469208776773517\n",
      "iteration 2000 / 75000: loss 5.84046734572323\n",
      "iteration 3000 / 75000: loss 7.671618959892193\n",
      "iteration 4000 / 75000: loss 2.779870180175286\n",
      "iteration 5000 / 75000: loss 4.508324116190355\n",
      "iteration 6000 / 75000: loss 6.243137024106769\n",
      "iteration 7000 / 75000: loss 5.091934392656745\n",
      "iteration 8000 / 75000: loss 3.021617383523456\n",
      "iteration 9000 / 75000: loss 1.6459575790330283\n",
      "iteration 10000 / 75000: loss 4.241049698323684\n",
      "iteration 11000 / 75000: loss 2.957367559972961\n",
      "iteration 12000 / 75000: loss 6.957825083600356\n",
      "iteration 13000 / 75000: loss 7.869371554142699\n",
      "iteration 14000 / 75000: loss 2.287048813675976\n",
      "iteration 15000 / 75000: loss 5.520541069981035\n",
      "iteration 16000 / 75000: loss 3.3314935320971255\n",
      "iteration 17000 / 75000: loss 0.4835014268790623\n",
      "iteration 18000 / 75000: loss 4.625005225091561\n",
      "iteration 19000 / 75000: loss 1.6211272526550842\n",
      "iteration 20000 / 75000: loss 2.8649155807829\n",
      "iteration 21000 / 75000: loss 2.445240588083564\n",
      "iteration 22000 / 75000: loss 2.25977390472356\n",
      "iteration 23000 / 75000: loss 3.5021494222716596\n",
      "iteration 24000 / 75000: loss 1.959346522754833\n",
      "iteration 25000 / 75000: loss 4.625093533021698\n",
      "iteration 26000 / 75000: loss 0.0\n",
      "iteration 27000 / 75000: loss 2.617844242590155\n",
      "iteration 28000 / 75000: loss 3.566700330240697\n",
      "iteration 29000 / 75000: loss 3.179329942259399\n",
      "iteration 30000 / 75000: loss 3.857412465887781\n",
      "iteration 31000 / 75000: loss 2.902294546896705\n",
      "iteration 32000 / 75000: loss 3.967247750399968\n",
      "iteration 33000 / 75000: loss 3.02023353209714\n",
      "iteration 34000 / 75000: loss 4.390266645508993\n",
      "iteration 35000 / 75000: loss 3.673060970029245\n",
      "iteration 36000 / 75000: loss 6.578714878816994\n",
      "iteration 37000 / 75000: loss 3.625946498147476\n",
      "iteration 38000 / 75000: loss 2.248082298636529\n",
      "iteration 39000 / 75000: loss 2.7246008178860888\n",
      "iteration 40000 / 75000: loss 0.3131629763161512\n",
      "iteration 41000 / 75000: loss 2.5980593064415616\n",
      "iteration 42000 / 75000: loss 0.7325196103393872\n",
      "iteration 43000 / 75000: loss 3.1082351607474714\n",
      "iteration 44000 / 75000: loss 2.858264359795937\n",
      "iteration 45000 / 75000: loss 4.280436125521076\n",
      "iteration 46000 / 75000: loss 2.863232443084664\n",
      "iteration 47000 / 75000: loss 5.366958319513458\n",
      "iteration 48000 / 75000: loss 0.44215349422668293\n",
      "iteration 49000 / 75000: loss 0.9879078237571786\n",
      "iteration 50000 / 75000: loss 2.1251135493517195\n",
      "iteration 51000 / 75000: loss 3.1285279435981366\n",
      "iteration 52000 / 75000: loss 6.426364878816793\n",
      "iteration 53000 / 75000: loss 2.0795475320346952\n",
      "iteration 54000 / 75000: loss 4.907023817325058\n",
      "iteration 55000 / 75000: loss 1.7759524603395223\n",
      "iteration 56000 / 75000: loss 1.561677399401446\n",
      "iteration 57000 / 75000: loss 2.274873501941475\n",
      "iteration 58000 / 75000: loss 3.741930560563148\n",
      "iteration 59000 / 75000: loss 2.173019712994851\n",
      "iteration 60000 / 75000: loss 3.241866025569786\n",
      "iteration 61000 / 75000: loss 0.8073330571547859\n",
      "iteration 62000 / 75000: loss 4.909026483805828\n",
      "iteration 63000 / 75000: loss 4.678675005132668\n",
      "iteration 64000 / 75000: loss 3.1959095218742153\n",
      "iteration 65000 / 75000: loss 4.881105003053804\n",
      "iteration 66000 / 75000: loss 0.18335528451161287\n",
      "iteration 67000 / 75000: loss 1.634275436656866\n",
      "iteration 68000 / 75000: loss 2.912977012045678\n",
      "iteration 69000 / 75000: loss 1.773132460340177\n",
      "iteration 70000 / 75000: loss 1.6440382807178657\n",
      "iteration 71000 / 75000: loss 1.7077669322214541\n",
      "iteration 72000 / 75000: loss 2.170791239077107\n",
      "iteration 73000 / 75000: loss 1.7135777448516905\n",
      "iteration 74000 / 75000: loss 2.030435954565565\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVM(fit_intercept=True)\n",
    "model.train(X, y, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X)\n",
    "model_accuracy = accuracy_score(y, pred)\n",
    "print(model_accuracy)\n",
    "assert model_accuracy > 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
